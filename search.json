[
  {
    "objectID": "pages/slides.html#a-bit-of-background-about-me-and-this-work",
    "href": "pages/slides.html#a-bit-of-background-about-me-and-this-work",
    "title": "Geospatial reprojection in Python",
    "section": "A bit of background about me and this work",
    "text": "A bit of background about me and this work",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#caveats",
    "href": "pages/slides.html#caveats",
    "title": "Geospatial reprojection in Python",
    "section": "Caveats",
    "text": "Caveats\n\nWork in progress!\nRecording will quickly become out-of-date\nVerify/fix code before use",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#definitions",
    "href": "pages/slides.html#definitions",
    "title": "Geospatial reprojection in Python",
    "section": "Definitions",
    "text": "Definitions\nReprojection - changing the projection of a dataset from one coordinate reference system (CRS) to another",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#definitions-1",
    "href": "pages/slides.html#definitions-1",
    "title": "Geospatial reprojection in Python",
    "section": "Definitions",
    "text": "Definitions\nResampling/regridding - changing the grid structure (often resolution)",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#definitions-2",
    "href": "pages/slides.html#definitions-2",
    "title": "Geospatial reprojection in Python",
    "section": "Definitions",
    "text": "Definitions\nWarp resampling - changing the resolution and projection of a dataset",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#grid-structures",
    "href": "pages/slides.html#grid-structures",
    "title": "Geospatial reprojection in Python",
    "section": "Grid structures",
    "text": "Grid structures\n\nRectilinear - described by one-dimensional latitude and longitude coordinates\n\nRegular - described by one x,y coordinate and the resolution\n\nCurvilinear - described by two-dimensional latitude and longitude coordinates\nUnstructured - Grids in which the grid coordinates require a list of nodes\n\n\nReview NCAR’s climate data guide for more information",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#resampling-algorithms",
    "href": "pages/slides.html#resampling-algorithms",
    "title": "Geospatial reprojection in Python",
    "section": "Resampling algorithms",
    "text": "Resampling algorithms\n\nNearest neighbor\nBilinear\nCubic\nSpline\nInverse distance\nBucket / binning (average, min, max, mode, med, quartile, sum, rms)\nSpectral\nTriangulation\nConservative\n\n\nReview NCAR’s climate data guide for more information",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#some-of-the-many-reasons-to-warp-resample",
    "href": "pages/slides.html#some-of-the-many-reasons-to-warp-resample",
    "title": "Geospatial reprojection in Python",
    "section": "Some of the many reasons to warp resample",
    "text": "Some of the many reasons to warp resample\nCo-registering datasets\n\nMosaicing\nStatistical analyses\nMachine learning\n\nVisualization\n\nRendering (minimize distortion)\nBuilding overviews",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#observations-and-opinions",
    "href": "pages/slides.html#observations-and-opinions",
    "title": "Geospatial reprojection in Python",
    "section": "Observations and opinions",
    "text": "Observations and opinions\n\nLots of kernels were killed in the making of this presentation\n\nwe need a demo using a bounded-memory approach (Cubed!)\n\nThere are some awesome data cube libraries in Python\n\nlet’s work with the developers to make them even better…and not build another one\n\nXarray’s data model is intuitive for a lot of people\n\nuse accessors to extend it’s functionality rather than a new data class",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#whats-next-for-the-guide",
    "href": "pages/slides.html#whats-next-for-the-guide",
    "title": "Geospatial reprojection in Python",
    "section": "What’s next for the guide",
    "text": "What’s next for the guide\n\nTry caching weights\nSmall tile from a large dataset\nAdd information about grid structures supported\nAdd information about resampling methods supported\nTest with virtualized data\nTest with cloud optimized data\nTest with other resampling algorithms",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#thanks",
    "href": "pages/slides.html#thanks",
    "title": "Geospatial reprojection in Python",
    "section": "Thanks",
    "text": "Thanks\n\nDevelopment Seed\nPangeo Community\n\nspecial thanks to Justus, Michael, and Deepak\n\nNASA IMPACT",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "pages/slides.html#whats-next-for-resampling-in-python",
    "href": "pages/slides.html#whats-next-for-resampling-in-python",
    "title": "Geospatial reprojection in Python",
    "section": "What’s next for resampling in Python",
    "text": "What’s next for resampling in Python\nLet’s discuss!",
    "crumbs": [
      "Overview Slides"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "This repository and the accompanying Quarto book contains examples of using warp resampling / reprojection methods in Python, along with memory and statistical wall-time profiling results.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "This repository and the accompanying Quarto book contains examples of using warp resampling / reprojection methods in Python, along with memory and statistical wall-time profiling results.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#memory-and-time-profiling",
    "href": "index.html#memory-and-time-profiling",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "Memory and time profiling",
    "text": "Memory and time profiling\nResampling and reprojection (i.e., warp resampling) are essential steps for generating raster tiles for browser based visualization. Further, warp resampling is often one of the most time consuming and memory intensive portions of the tile generation process. The importance and complexity of this step motivates an exploration of different warp resampling options.\n\nGoals\nCompare memory and time performance for generating Web Mercator Quad Tree raster tiles from one timestep and variable of the MUR SST and GPM IMERG dataset using the following approaches:\n\nosgeo.warp\nrasterio.warp.reproject\nrioxarray.reproject\npyresample.resample_blocks\nxesmf.Regridder\ngeoutils.Raster.reproject\nraster_tools.warp.reproject\nodc.geo.xr.xr_reproject\nxcube.resampling\ngeowombat.config.update\n\nOut-of-scope:\n\nxarray-regrid - only regrids within the same rectilinear coordinate system\nweatherbench2/regridding - only seems to regrid within the same rectilinear coordinate system\ndinosaur - only seems to regrid within the same rectilinear coordinate system\npygmt.grdproject - web mercator not amongst supported projections\nverde - not used for raster -&gt; raster resampling (only points -&gt; raster)\n\nThese methods will be run on the full resolution dataset. Nearest neighbor interpolation will be used for the first comparison. For simplicity, the amount of time necessary to generate a resampled array and the maximum amount of heap memory allocated will be measured. We also compare results when using a virtual dataset (e.g., VRT, Kerchunk reference file), when reading from a dataset stored locally versus in cloud object storage, and when using a cloud-optimized dataset (Zarr.)\n\n\nPossible extensions\n\nCompare other resampling methods (e.g., bilinear, conservative).\nCompare with methods that don’t rely on existing packages (e.g., Conservative regridding with Xarray, GeoPandas, and Sparse and KDTree wrappers).\n\n\n\nEnvironment Setup\nThe notebooks can be run on a JupyterHub environment using the docker image quay.io/developmentseed/warp-resample-profiling:latest, which is created using repo2docker using the Dockerfile contained within the binder directory.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "References",
    "text": "References\n\nWhat’s Next - Software - Regridding\nLazy regridding discussion\n\n\nAcknowledgements\nThis work was made possible through support from NASA IMPACT. Numerous people have guided development, especially Aimee Barciauskas (@abarciauskas-bgse), Justus Magin (@keewis), and Michael Sumner (@mdsumner). The resources page contains references for source information. Quarto configuration based on Cloud Native Geospatial Formats Guide and the Tile Benchmarking. All mistakes my own.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "examples/run-memray.html",
    "href": "examples/run-memray.html",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "import subprocess\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport fsspec\nfrom utils import sync_notebook\n\n\nfs = fsspec.filesystem(\"file\")\ncurrent_date = datetime.today().strftime(\"%Y-%m-%d\")\noutput_folder = f\"results/{current_date}/\"\nfs.mkdirs(output_folder, exist_ok=True)\n\n\ndataset = \"gpm_imerg\"\n\n\ndef run_memray(file: str, zoom: str):\n    output_file = f\"{output_folder}memray-{dataset}-{Path(file).stem}-{zoom}.bin\"\n    summary_file = f\"{output_folder}memray-{dataset}-{Path(file).stem}-{zoom}.json\"\n    command = [\n        \"memray\",\n        \"run\",\n        \"--force\",\n        \"--output\",\n        output_file,\n        file,\n        \"--dataset\",\n        dataset,\n        \"--zoom\",\n        zoom,\n    ]\n    subprocess.run(command)\n    summary_command = [\n        \"memray\",\n        \"stats\",\n        \"--force\",\n        \"--json\",\n        \"--output\",\n        summary_file,\n        output_file,\n    ]\n    subprocess.run(summary_command)\n\n\ndef sync_and_run(zoom, input_methods):\n    notebooks = []\n    for fp in input_methods:\n        notebooks.extend(fs.glob(f\"resample-*-{fp}*.ipynb\"))\n    for file in notebooks:\n        sync_notebook(file)\n    modules = []\n    for fp in input_methods:\n        modules.extend(fs.glob(f\"resample-*-{fp}*.py\"))\n    for file in modules:\n        # Skip kerchunk since it requires a different image build due to incompatibility with Zarr V3\n        if \"kerchunk\" not in file:\n            run_memray(file, zoom)\n\n\nif dataset == \"gpm_imerg\":\n    input_methods = [\"rioxarray\", \"odc\", \"pyresample\", \"rioxarray\", \"xesmf\", \"sparse\"]\n    zoom_levels = [\"0\", \"1\", \"2\"]\nelif dataset == \"mursst\":\n    input_methods = [\"odc\", \"pyresample\", \"rioxarray\", \"rasterio\"]\n    zoom_levels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n\n\nfor zoom in zoom_levels:\n    sync_and_run(zoom, input_methods)"
  },
  {
    "objectID": "examples/resample-zarr-xesmf-zarr-icechunk.html",
    "href": "examples/resample-zarr-xesmf-zarr-icechunk.html",
    "title": "Resampling with XESMF (S3 storage, Zarr V3 store, Zarr reader with icechunk)¶",
    "section": "",
    "text": "import argparse\nimport itertools\n\nimport numpy as np\nimport pyproj\nimport rasterio.transform\nimport xarray as xr\nimport xesmf as xe\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef make_grid_ds(*, te, tilesize, dstSRS) -&gt; xr.Dataset:\n    \"\"\"\n    Make a dataset representing a target grid\n\n    Returns\n    -------\n    xr.Dataset\n        Target grid dataset with the following variables:\n        - \"x\": X coordinate in Web Mercator projection (grid cell center)\n        - \"y\": Y coordinate in Web Mercator projection (grid cell center)\n        - \"lat\": latitude coordinate (grid cell center)\n        - \"lon\": longitude coordinate (grid cell center)\n        - \"lat_b\": latitude bounds for grid cell\n        - \"lon_b\": longitude bounds for grid cell\n\n    Notes\n    -----\n    Modified from ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    transform = rasterio.transform.Affine.translation(\n        te[0], te[3]\n    ) * rasterio.transform.Affine.scale((te[2] * 2) / tilesize, (te[1] * 2) / tilesize)\n\n    p = pyproj.Proj(dstSRS)\n\n    grid_shape = (tilesize, tilesize)\n    bounds_shape = (tilesize + 1, tilesize + 1)\n\n    xs = np.empty(grid_shape)\n    ys = np.empty(grid_shape)\n    lat = np.empty(grid_shape)\n    lon = np.empty(grid_shape)\n    lat_b = np.zeros(bounds_shape)\n    lon_b = np.zeros(bounds_shape)\n\n    # calc grid cell center coordinates\n    ii, jj = np.meshgrid(np.arange(tilesize) + 0.5, np.arange(tilesize) + 0.5)\n    for i, j in itertools.product(range(grid_shape[0]), range(grid_shape[1])):\n        locs = [ii[i, j], jj[i, j]]\n        xs[i, j], ys[i, j] = transform * locs\n        lon[i, j], lat[i, j] = p(xs[i, j], ys[i, j], inverse=True)\n\n    # calc grid cell bounds\n    iib, jjb = np.meshgrid(np.arange(tilesize + 1), np.arange(tilesize + 1))\n    for i, j in itertools.product(range(bounds_shape[0]), range(bounds_shape[1])):\n        locs = [iib[i, j], jjb[i, j]]\n        x, y = transform * locs\n        lon_b[i, j], lat_b[i, j] = p(x, y, inverse=True)\n\n    return xr.Dataset(\n        {\n            \"x\": xr.DataArray(xs[0, :], dims=[\"x\"]),\n            \"y\": xr.DataArray(ys[:, 0], dims=[\"y\"]),\n            \"lat\": xr.DataArray(lat, dims=[\"y\", \"x\"]),\n            \"lon\": xr.DataArray(lon, dims=[\"y\", \"x\"]),\n            \"lat_b\": xr.DataArray(lat_b, dims=[\"y_b\", \"x_b\"]),\n            \"lon_b\": xr.DataArray(lon_b, dims=[\"y_b\", \"x_b\"]),\n        },\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Create grid to hold result\n    target_grid = make_grid_ds(te=te, tilesize=256, dstSRS=\"EPSG:3857\")\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        target_grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n    )\n    # Regrid dataset\n    return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-zarr-rioxarray-zarr-icechunk.html",
    "href": "examples/resample-zarr-rioxarray-zarr-icechunk.html",
    "title": "Resampling with Rioxarray (S3 storage, Zarr V3 store, Zarr reader with icechunk)",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom rasterio.warp import calculate_default_transform\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    if dataset == \"gpm_imerg\":\n        # Transpose and rename dims to align with rioxarray expectations\n        da = da.rename({\"lon\": \"x\", \"lat\": \"y\"})\n    # Set input dataset projection\n    da = da.rio.write_crs(srcSRS)\n    da = da.rio.clip_box(\n        *te,\n        crs=dstSRS,\n    )\n    # Define affine transformation from input to output dataset\n    dst_transform, w, h = calculate_default_transform(\n        srcSRS,\n        dstSRS,\n        da.rio.width,\n        da.rio.height,\n        *da.rio.bounds(),\n        dst_width=width,\n        dst_height=height,\n    )\n    # Reproject dataset\n    return da.rio.reproject(dstSRS, shape=(h, w), transform=dst_transform)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rioxarray",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-zarr-odc-zarr-icechunk.html",
    "href": "examples/resample-zarr-odc-zarr-icechunk.html",
    "title": "Resampling with ODC-geo (S3 storage, Zarr V3 store, Zarr reader with icechunk)",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom odc.geo.geobox import GeoBox\nfrom odc.geo.geom import Geometry\nfrom odc.geo.xr import crop, xr_reproject\nfrom shapely.geometry import box\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Define ODC geobox for target tile\n    gbox = GeoBox.from_bbox(te, dstSRS, shape=(height, width))\n    if dataset == \"gpm_imerg\":\n        # Transpose and rename dataset dims to align with GDAL expectations\n        da = da.rename({\"lon\": \"x\", \"lat\": \"y\"}).squeeze()\n    # Assign input projection\n    da = da.odc.assign_crs(srcSRS)\n    # Crop dataset to tile bounds\n    bbox = box(*te)\n    geom = Geometry(bbox, \"EPSG:3857\")\n    da = crop(da, geom)\n    # Load into memory to avoid topology error\n    da.load()\n    # Reproject dataset\n    return xr_reproject(da, gbox, tight=True).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Open Data Cube",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-weboptimizedzarr-rasterio-zarr-icechunk.html",
    "href": "examples/resample-weboptimizedzarr-rasterio-zarr-icechunk.html",
    "title": "Resampling with Rasterio (S3 storage, Web-Optimized Zarr V3 store, Zarr reader with icechunk)",
    "section": "",
    "text": "WARNING: This notebook is intented to show the potential for web-optimized Zarrs that contain overviews, but the approach should not be used in production due to the lack a robust approach following a metadata specification.\n\nimport argparse\nimport os\n\nimport numpy as np\nimport zarr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom rasterio.crs import CRS\nfrom rasterio.transform import from_bounds\nfrom rasterio.warp import calculate_default_transform, reproject\n\n\n# Define filepath, driver, and variable information\ndataset = \"mursst\"\ndst = f\"{os.getcwd()}/earthaccess_data/{dataset}-overviews.zarr\"\n# Open dataset\nstorage_config = StorageConfig.filesystem(dst)\nstore = IcechunkStore.open_or_create(storage=storage_config)\n\n\ndef get_overview_level(\n    dataset_bounds,\n    dataset_shape,\n    target_bounds: tuple,\n    overviews: list,\n    height: int = 256,\n    width: int = 256,\n    srcSRS: CRS = CRS.from_string(\"EPSG:4326\"),\n    dstSRS: CRS = CRS.from_string(\"EPSG:3857\"),\n) -&gt; int:\n    \"\"\"Return the overview level corresponding to the tile resolution.\n\n    Freely adapted from rio-tiler, which freely adapted from https://github.com/OSGeo/gdal/blob/41993f127e6e1669fbd9e944744b7c9b2bd6c400/gdal/apps/gdalwarp_lib.cpp#L2293-L2362\n\n    Args:\n        src_dst (rasterio.io.DatasetReader or rasterio.io.DatasetWriter or rasterio.vrt.WarpedVRT): Rasterio dataset.\n        bounds (tuple): Bounding box coordinates in target crs (**dstSRS**).\n        overviews (list): List of overview decimation levels.\n        height (int): Desired output height of the array for the input bounds.\n        width (int): Desired output width of the array for the input bounds.\n        srcSRS (rasterio.crs.CRS, optional): Source Coordinate Reference System. Defaults to `epsg:4326`.\n        dstSRS (rasterio.crs.CRS, optional): Target Coordinate Reference System. Defaults to `epsg:3857`.\n\n    Returns:\n        int: Overview level.\n\n    \"\"\"\n\n    dst_transform, _, _ = calculate_default_transform(\n        srcSRS, dstSRS, dataset_shape[1], dataset_shape[0], *dataset_bounds\n    )\n    src_res = dst_transform.a\n\n    # Compute what the \"natural\" output resolution\n    # (in pixels) would be for this input dataset\n    vrt_transform = from_bounds(*target_bounds, width, height)\n    target_res = vrt_transform.a\n\n    ovr_idx = -1\n    if target_res &gt; src_res:\n        res = [src_res * decim for decim in overviews]\n\n        for idx in range(ovr_idx, len(res) - 1):\n            ovr_idx = idx\n            ovrRes = src_res if ovr_idx &lt; 0 else res[ovr_idx]\n            nextRes = res[ovr_idx + 1]\n\n            if (ovrRes &lt; target_res) and (nextRes &gt; target_res):\n                break\n\n            if abs(ovrRes - target_res) &lt; 1e-1:\n                break\n\n        else:\n            print(\"else\")\n            ovr_idx = len(res) - 1\n    return overviews[ovr_idx - 1]\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import target_extent\n\n    te = target_extent[zoom]\n\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-overviews.zarr\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_or_create(storage=storage)\n\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Hard code some metadata that could be included in a GeoZarr spec\n\n    bounds = [-179.995, -89.99499999999999, 180.005, 89.99499999999999]\n    shape = (17999, 36000)\n    overviews = [2, 4, 8, 16, 32, 64]\n    affine = {\n        2: {\n            \"bounds\": (-179.995, -89.99500000000002, 180.005, 89.99499999999999),\n            \"shape\": (9000, 18000),\n        },\n        4: {\n            \"bounds\": (-179.995, -89.99500000000002, 180.005, 89.99499999999999),\n            \"shape\": (4500, 9000),\n        },\n        8: {\n            \"bounds\": (-179.995, -89.99499999999999, 180.005, 89.99499999999999),\n            \"shape\": (2250, 4500),\n        },\n        16: {\n            \"bounds\": (\n                -179.995,\n                -89.99500000000002,\n                180.00500000000005,\n                89.99499999999999,\n            ),\n            \"shape\": (1125, 2250),\n        },\n        32: {\n            \"bounds\": (-179.995, -89.99500000000002, 180.005, 89.99499999999999),\n            \"shape\": (563, 1125),\n        },\n        64: {\n            \"bounds\": (-179.995, -89.99500000000002, 180.005, 89.99499999999999),\n            \"shape\": (282, 563),\n        },\n    }\n    # Get overview level for associated zoom level\n    level = get_overview_level(bounds, shape, te, overviews)\n    # Open overview\n    data = zarr.open(store, mode=\"r\", path=f\"{level}/var\")\n    # Define affine transformation from input to output dataset\n    src_transform = from_bounds(\n        *affine[level][\"bounds\"], affine[level][\"shape\"][1], affine[level][\"shape\"][0]\n    )\n    dst_transform = from_bounds(*te, width, height)\n    # Create array to host results\n    destination = np.zeros((height, width), np.float32)\n    # Reproject dataset\n    _, transform = reproject(\n        np.squeeze(data),\n        destination,\n        src_crs=srcSRS,\n        src_transform=src_transform,\n        dst_crs=dstSRS,\n        dst_transform=dst_transform,\n    )\n    return destination\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))"
  },
  {
    "objectID": "examples/resample-netcdf-xesmfcached-zarr-icechunk.html",
    "href": "examples/resample-netcdf-xesmfcached-zarr-icechunk.html",
    "title": "Resampling with XESMF (S3 storage, NetCDF file, Zarr reader, icechunk virtualization, and pre-generated weights)",
    "section": "",
    "text": "import argparse\n\nimport numpy as np\nimport xarray as xr\nimport xesmf as xe\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args  # noqa: 402\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-weights-{zoom}\",\n        region=\"us-west-2\",\n    )\n    target_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-target-{zoom}\",\n        region=\"us-west-2\",\n    )\n    weights_store = IcechunkStore.open_existing(storage=weights_storage, mode=\"r\")\n    target_store = IcechunkStore.open_existing(storage=target_storage, mode=\"r\")\n    weights = _reconstruct_xesmf_weights(\n        xr.open_zarr(weights_store, zarr_format=3, consolidated=False)\n    )\n    grid = xr.open_zarr(target_store, zarr_format=3, consolidated=False)\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-reference\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n        reuse_weights=True,\n        weights=weights,\n    )\n    # Regrid dataset\n    return regridder(da)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "Zarr Reader + Icechunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmfcached-h5netcdf-.html",
    "href": "examples/resample-netcdf-xesmfcached-h5netcdf-.html",
    "title": "Resampling with XESMF (S3 storage, NetCDF file, H5NetCDF driver, earthaccess auth, and pre-generated weights)",
    "section": "",
    "text": "import argparse\n\nimport earthaccess\nimport numpy as np\nimport xarray as xr\nimport xesmf as xe\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef reconstruct_weights(weights_fp):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    return _reconstruct_xesmf_weights(xr.open_zarr(weights_fp))\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args  # noqa: 402\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-weights-{zoom}.zarr\"\n    target_grid_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-target-{zoom}.zarr\"\n    weights = reconstruct_weights(weights_fp)\n    grid = xr.open_zarr(target_grid_fp)\n    # Define filepath, driver, and variable information\n    input_uri = f'{args[\"folder\"]}/{args[\"filename\"]}'\n    src = f's3://{args[\"bucket\"]}/{input_uri}'\n    # Authenticate with earthaccess\n    fs = earthaccess.get_s3_filesystem(daac=args[\"daac\"])\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        # Create XESMF regridder\n        regridder = xe.Regridder(\n            da,\n            grid,\n            \"nearest_s2d\",\n            periodic=True,\n            extrap_method=\"nearest_s2d\",\n            ignore_degenerate=True,\n            reuse_weights=True,\n            weights=weights,\n        )\n        # Regrid dataset\n        return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "H5NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmf-h5netcdf-local.html",
    "href": "examples/resample-netcdf-xesmf-h5netcdf-local.html",
    "title": "Resampling with XESMF (local storage, NetCDF file, H5NetCDF driver)",
    "section": "",
    "text": "import argparse\nimport itertools\n\nimport fsspec\nimport numpy as np\nimport pyproj\nimport rasterio.transform\nimport xarray as xr\nimport xesmf as xe\n\n\ndef make_grid_ds(*, te, tilesize, dstSRS) -&gt; xr.Dataset:\n    \"\"\"\n    Make a dataset representing a target grid\n\n    Returns\n    -------\n    xr.Dataset\n        Target grid dataset with the following variables:\n        - \"x\": X coordinate in Web Mercator projection (grid cell center)\n        - \"y\": Y coordinate in Web Mercator projection (grid cell center)\n        - \"lat\": latitude coordinate (grid cell center)\n        - \"lon\": longitude coordinate (grid cell center)\n        - \"lat_b\": latitude bounds for grid cell\n        - \"lon_b\": longitude bounds for grid cell\n\n    Notes\n    -----\n    Modified from ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    transform = rasterio.transform.Affine.translation(\n        te[0], te[3]\n    ) * rasterio.transform.Affine.scale((te[2] * 2) / tilesize, (te[1] * 2) / tilesize)\n\n    p = pyproj.Proj(dstSRS)\n\n    grid_shape = (tilesize, tilesize)\n    bounds_shape = (tilesize + 1, tilesize + 1)\n\n    xs = np.empty(grid_shape)\n    ys = np.empty(grid_shape)\n    lat = np.empty(grid_shape)\n    lon = np.empty(grid_shape)\n    lat_b = np.zeros(bounds_shape)\n    lon_b = np.zeros(bounds_shape)\n\n    # calc grid cell center coordinates\n    ii, jj = np.meshgrid(np.arange(tilesize) + 0.5, np.arange(tilesize) + 0.5)\n    for i, j in itertools.product(range(grid_shape[0]), range(grid_shape[1])):\n        locs = [ii[i, j], jj[i, j]]\n        xs[i, j], ys[i, j] = transform * locs\n        lon[i, j], lat[i, j] = p(xs[i, j], ys[i, j], inverse=True)\n\n    # calc grid cell bounds\n    iib, jjb = np.meshgrid(np.arange(tilesize + 1), np.arange(tilesize + 1))\n    for i, j in itertools.product(range(bounds_shape[0]), range(bounds_shape[1])):\n        locs = [iib[i, j], jjb[i, j]]\n        x, y = transform * locs\n        lon_b[i, j], lat_b[i, j] = p(x, y, inverse=True)\n\n    return xr.Dataset(\n        {\n            \"x\": xr.DataArray(xs[0, :], dims=[\"x\"]),\n            \"y\": xr.DataArray(ys[:, 0], dims=[\"y\"]),\n            \"lat\": xr.DataArray(lat, dims=[\"y\", \"x\"]),\n            \"lon\": xr.DataArray(lon, dims=[\"y\", \"x\"]),\n            \"lat_b\": xr.DataArray(lat_b, dims=[\"y_b\", \"x_b\"]),\n            \"lon_b\": xr.DataArray(lon_b, dims=[\"y_b\", \"x_b\"]),\n        },\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information    args = earthaccess_args[dataset]\n    args = earthaccess_args[dataset]\n    src = f'earthaccess_data/{args[\"filename\"]}'\n    # Create grid to hold result\n    target_grid = make_grid_ds(te=te, tilesize=256, dstSRS=\"EPSG:3857\")\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    fs = fsspec.filesystem(\"file\")\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={}, mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        # Create XESMF regridder\n        regridder = xe.Regridder(\n            da,\n            target_grid,\n            \"nearest_s2d\",\n            periodic=True,\n            extrap_method=\"nearest_s2d\",\n            ignore_degenerate=True,\n        )\n        # Regrid dataset\n        return regridder(da).load()\n\n\n%%time\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF",
      "H5NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-rioxarray-zarr-icechunk.html",
    "href": "examples/resample-netcdf-rioxarray-zarr-icechunk.html",
    "title": "Resampling with Rioxarray (S3 storage, NetCDF file, Zarr reader, icechunk virtualization)",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom rasterio.warp import calculate_default_transform\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-reference\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    if dataset == \"gpm_imerg\":\n        # Transpose and rename dims to align with rioxarray expectations\n        da = da.rename({\"lon\": \"x\", \"lat\": \"y\"}).transpose(\"time\", \"y\", \"x\")\n    # Set input dataset projection\n    da = da.rio.write_crs(srcSRS)\n    da = da.rio.clip_box(\n        *te,\n        crs=dstSRS,\n    )\n    # Define affine transformation from input to output dataset\n    dst_transform, w, h = calculate_default_transform(\n        srcSRS,\n        dstSRS,\n        da.rio.width,\n        da.rio.height,\n        *da.rio.bounds(),\n        dst_width=width,\n        dst_height=height,\n    )\n    # Reproject dataset\n    return da.rio.reproject(dstSRS, shape=(h, w), transform=dst_transform)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rioxarray",
      "Zarr Reader + Icechunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-rioxarray-h5netcdf-.html",
    "href": "examples/resample-netcdf-rioxarray-h5netcdf-.html",
    "title": "Resampling with Rioxarray (S3 storage, NetCDF file, H5NetCDF driver, earthaccess auth)",
    "section": "",
    "text": "import argparse\n\nimport earthaccess\nimport xarray as xr\nfrom rasterio.warp import calculate_default_transform\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    input_uri = f'{args[\"folder\"]}/{args[\"filename\"]}'\n    src = f's3://{args[\"bucket\"]}/{input_uri}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Authenticate with earthaccess\n    fs = earthaccess.get_s3_filesystem(daac=args[\"daac\"])\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        if dataset == \"gpm_imerg\":\n            # Transpose and rename dims to align with rioxarray expectations\n            da = da.rename({\"lon\": \"x\", \"lat\": \"y\"}).transpose(\"time\", \"y\", \"x\")\n        # Set input dataset projection\n        da = da.rio.write_crs(srcSRS)\n        da = da.rio.clip_box(\n            *te,\n            crs=dstSRS,\n        )\n        # Define affine transformation from input to output dataset\n        dst_transform, w, h = calculate_default_transform(\n            srcSRS,\n            dstSRS,\n            da.rio.width,\n            da.rio.height,\n            *da.rio.bounds(),\n            dst_width=width,\n            dst_height=height,\n        )\n        # Reproject dataset\n        return da.rio.reproject(dstSRS, shape=(h, w), transform=dst_transform)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rioxarray",
      "H5NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-rasterio-netcdf-local.html",
    "href": "examples/resample-netcdf-rasterio-netcdf-local.html",
    "title": "Resampling with rasterio (Local storage, NetCDF File, NetCDF4 driver)",
    "section": "",
    "text": "import argparse\nimport json\n\nimport geopandas as gpd\nimport numpy as np\nimport rasterio\nfrom rasterio.mask import mask\nfrom rasterio.warp import reproject\nfrom shapely.geometry import box\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    src = f'NETCDF:earthaccess_data/{args[\"filename\"]}:{args[\"variable\"]}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    with rasterio.open(src) as da:\n        # Clip dataset to bounds of Web Mercator tile\n        bbox = box(*te)\n        geo = gpd.GeoDataFrame(\n            {\"geometry\": bbox}, index=[0], crs=int(dstSRS.split(\":\")[1])\n        )\n        geo = geo.to_crs(crs=srcSRS)\n        coords = [json.loads(geo.to_json())[\"features\"][0][\"geometry\"]]\n        arr, src_transform = mask(da, shapes=coords, crop=True)\n        # Mask and fill array\n        ma = arr.astype(\"float32\", casting=\"unsafe\")\n        np.multiply(ma, da.scales[0], out=ma, casting=\"unsafe\")\n        np.add(ma, da.offsets[0], out=ma, casting=\"unsafe\")\n        # Define affine transformation from input to output dataset\n        dst_transform = rasterio.transform.from_bounds(*te, width, height)\n        # Create array to host results\n        destination = np.zeros((height, width), np.float32)\n        # Reproject dataset\n        _, transform = reproject(\n            ma.squeeze(),\n            destination,\n            src_crs=srcSRS,\n            src_transform=src_transform,\n            dst_crs=dstSRS,\n            dst_transform=dst_transform,\n        )\n        return destination\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rasterio",
      "NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-pyresample-h5netcdf-local.html",
    "href": "examples/resample-netcdf-pyresample-h5netcdf-local.html",
    "title": "Resampling with pyresample (local storage, NetCDF File, H5NetCDF driver)",
    "section": "",
    "text": "import argparse\n\nimport fsspec\nimport xarray as xr\nfrom pyresample.area_config import create_area_def\nfrom pyresample.gradient import block_nn_interpolator, gradient_resampler_indices_block\nfrom pyresample.resampler import resample_blocks\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    src = f'earthaccess_data/{args[\"filename\"]}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    fs = fsspec.filesystem(\"file\")\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={})[args[\"variable\"]]\n        # Rechunk MURSST to operate on fewer chunks\n        if dataset == \"mursst\":\n            da = da.chunk({\"time\": -1, \"lat\": 4000, \"lon\": 4000})\n        elif dataset == \"gpm_imerg\":\n            # Transpose dims to align with pyresample expectations\n            da = da.transpose(\"time\", \"lat\", \"lon\").squeeze()\n        # Create area definition for the target dataset\n        target_area_def = create_area_def(\n            area_id=1,\n            projection=dstSRS,\n            shape=(height, width),\n            area_extent=te,\n        )\n        # Create area definition for the source dataset\n        source_area_def = create_area_def(\n            area_id=2,\n            projection=srcSRS,\n            shape=(da.sizes[\"lat\"], da.sizes[\"lon\"]),\n            area_extent=[-179.995, 89.995, 180.005, -89.995],\n        )\n        # Compute indices for resampling\n        indices_xy = resample_blocks(\n            gradient_resampler_indices_block,\n            source_area_def,\n            [],\n            target_area_def,\n            chunk_size=(1, height, width),\n            dtype=float,\n        )\n        # Apply resampler\n        resampled = resample_blocks(\n            block_nn_interpolator,\n            source_area_def,\n            [da.data],\n            target_area_def,\n            dst_arrays=[indices_xy],\n            chunk_size=(1, height, width),\n            dtype=da.dtype,\n        )\n        # Reproject dataset\n        return resampled.compute()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Pyresample",
      "H5NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-odc-zarr-icechunk.html",
    "href": "examples/resample-netcdf-odc-zarr-icechunk.html",
    "title": "Resampling with ODC-geo (S3 storage, NetCDF file, Zarr reader, icechunk virtualization)",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom odc.geo.geobox import GeoBox\nfrom odc.geo.geom import Geometry\nfrom odc.geo.xr import crop, xr_reproject\nfrom shapely.geometry import box\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-reference\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Define ODC geobox for target tile\n    gbox = GeoBox.from_bbox(te, dstSRS, shape=(height, width))\n    if dataset == \"gpm_imerg\":\n        # Transpose and rename dataset dims to align with GDAL expectations\n        da = da.rename({\"lon\": \"x\", \"lat\": \"y\"}).transpose(\"time\", \"y\", \"x\").squeeze()\n    # Assign input projection\n    da = da.odc.assign_crs(srcSRS)\n    # Crop dataset to tile bounds\n    bbox = box(*te)\n    geom = Geometry(bbox, \"EPSG:3857\")\n    da = crop(da, geom)\n    # Load into memory to avoid topology error\n    da.load()\n    # Reproject dataset\n    return xr_reproject(da, gbox, tight=True).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Open Data Cube",
      "Zarr Reader + Icechunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-odc-h5netcdf-.html",
    "href": "examples/resample-netcdf-odc-h5netcdf-.html",
    "title": "Resampling with ODC-geo (s3 storage, NetCDF File, H5NetCDF driver, earthaccess auth)",
    "section": "",
    "text": "import argparse\n\nimport earthaccess\nimport xarray as xr\nfrom odc.geo.geobox import GeoBox\nfrom odc.geo.geom import Geometry\nfrom odc.geo.xr import crop, xr_reproject\nfrom shapely.geometry import box\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    input_uri = f'{args[\"folder\"]}/{args[\"filename\"]}'\n    src = f's3://{args[\"bucket\"]}/{input_uri}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Authentical via earthaccess\n    earthaccess.login()\n    fs = earthaccess.get_s3_filesystem(daac=args[\"daac\"])\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Define ODC geobox for target tile\n        gbox = GeoBox.from_bbox(te, dstSRS, shape=(height, width))\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={})[args[\"variable\"]]\n        if dataset == \"gpm_imerg\":\n            # Transpose and rename dataset dims to align with GDAL expectations\n            da = (\n                da.rename({\"lon\": \"x\", \"lat\": \"y\"})\n                .transpose(\"time\", \"y\", \"x\")\n                .squeeze()\n            )\n        # Assign input projection\n        da = da.odc.assign_crs(srcSRS)\n        # Crop dataset to tile bounds\n        bbox = box(*te)\n        geom = Geometry(bbox, \"EPSG:3857\")\n        da = crop(da, geom)\n        # Load into memory to avoid topology error\n        da.load()\n        # Reproject dataset\n        return xr_reproject(da, gbox, tight=True).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Open Data Cube",
      "H5NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-cog-rasterio-cog-.html",
    "href": "examples/resample-cog-rasterio-cog-.html",
    "title": "Resampling with rasterio (Local storage, COG file, COG driver)",
    "section": "",
    "text": "import argparse\n\nimport numpy as np\nimport rasterio\nfrom rasterio.vrt import WarpedVRT\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import target_extent\n\n    te = target_extent[zoom]\n\n    # Define source and target projection\n    srcSRS = \"EPSG:4326\"\n    dstSRS = \"EPSG:3857\"\n    width = height = 256\n\n    src = f\"s3://nasa-veda-scratch/resampling/{dataset}.tif\"\n\n    with rasterio.open(src) as da:\n        with WarpedVRT(da, src_crs=srcSRS, crs=dstSRS) as vrt:\n\n            dst_window = vrt.window(*te)\n\n            data = vrt.read(window=dst_window, out_shape=(height, width), masked=True)\n            # Mask and fill array\n            data = data.astype(\"float32\", casting=\"unsafe\")\n            np.multiply(data, da.scales[0], out=data, casting=\"unsafe\")\n            np.add(data, da.offsets[0], out=data, casting=\"unsafe\")\n            return data\n\n\n%%time\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))"
  },
  {
    "objectID": "examples/process-gpm-results.html",
    "href": "examples/process-gpm-results.html",
    "title": "Process results for GPM IMERG",
    "section": "",
    "text": "import hvplot.pandas  # noqa\nfrom utils import process_results\ndf = process_results(\"results/2024-10-25\").sort_values([\"duration (s)\"])",
    "crumbs": [
      "Profiling results",
      "Memory and time usage (GPM IMERG)"
    ]
  },
  {
    "objectID": "examples/process-gpm-results.html#show-memory-and-time-for-warp-resampling-dataset",
    "href": "examples/process-gpm-results.html#show-memory-and-time-for-warp-resampling-dataset",
    "title": "Process results for GPM IMERG",
    "section": "Show memory and time for warp resampling dataset",
    "text": "Show memory and time for warp resampling dataset\n\ndf = df[df[\"task\"] == \"resample\"]\ndf = df[df[\"dataset\"] == \"gpm_imerg\"]\ndf[df[\"zoom\"] == \"0\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n12\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n0\n0.510845\n2.825584\n\n\n3\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n0\n0.219070\n2.873800\n\n\n51\ngpm_imerg\nresample\nzarr\nrioxarray\nzarr\nicechunk\n0\n0.214933\n3.248126\n\n\n48\ngpm_imerg\nresample\nzarr\npyresample\nzarr\nicechunk\n0\n0.484800\n3.420104\n\n\n24\ngpm_imerg\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n0\n0.213210\n3.432233\n\n\n6\ngpm_imerg\nresample\nnetcdf\nodc\nzarr\nicechunk\n0\n0.218082\n3.527490\n\n\n45\ngpm_imerg\nresample\nzarr\nodc\nzarr\nicechunk\n0\n0.219883\n3.595781\n\n\n15\ngpm_imerg\nresample\nnetcdf\npyresample\nzarr\nicechunk\n0\n0.509712\n3.631176\n\n\n21\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n0\n1.133411\n4.130201\n\n\n39\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\nlocal\n0\n0.424940\n5.035078\n\n\n54\ngpm_imerg\nresample\nzarr\nsparse\nzarr\nicechunk\n0\n0.168040\n6.213787\n\n\n0\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\n\n0\n0.238359\n8.207994\n\n\n36\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\n\n0\n0.436165\n9.760592\n\n\n60\ngpm_imerg\nresample\nzarr\nxesmfcached\nzarr\nicechunk\n0\n0.402360\n10.216686\n\n\n42\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nzarr\nicechunk\n0\n0.402591\n10.589594\n\n\n18\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n0\n1.153169\n10.647312\n\n\n9\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\n\n0\n0.505636\n11.401241\n\n\n30\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\nlocal\n0\n1.001367\n39.583973\n\n\n57\ngpm_imerg\nresample\nzarr\nxesmf\nzarr\nicechunk\n0\n0.997985\n39.985007\n\n\n33\ngpm_imerg\nresample\nnetcdf\nxesmf\nzarr\nicechunk\n0\n0.999235\n41.595189\n\n\n27\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\n\n0\n1.021253\n48.698868\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"1\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n13\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n1\n0.238092\n2.623957\n\n\n4\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n1\n0.153546\n2.728585\n\n\n22\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n1\n0.361287\n2.744986\n\n\n52\ngpm_imerg\nresample\nzarr\nrioxarray\nzarr\nicechunk\n1\n0.140098\n3.236913\n\n\n25\ngpm_imerg\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n1\n0.158653\n3.243511\n\n\n49\ngpm_imerg\nresample\nzarr\npyresample\nzarr\nicechunk\n1\n0.211257\n3.273793\n\n\n46\ngpm_imerg\nresample\nzarr\nodc\nzarr\nicechunk\n1\n0.144622\n3.343807\n\n\n7\ngpm_imerg\nresample\nnetcdf\nodc\nzarr\nicechunk\n1\n0.161910\n3.438085\n\n\n16\ngpm_imerg\nresample\nnetcdf\npyresample\nzarr\nicechunk\n1\n0.228543\n3.451255\n\n\n40\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\nlocal\n1\n0.424828\n4.936243\n\n\n55\ngpm_imerg\nresample\nzarr\nsparse\nzarr\nicechunk\n1\n0.167936\n5.932873\n\n\n19\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n1\n0.382223\n8.025317\n\n\n1\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\n\n1\n0.172499\n8.083535\n\n\n10\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\n\n1\n0.239817\n8.201538\n\n\n61\ngpm_imerg\nresample\nzarr\nxesmfcached\nzarr\nicechunk\n1\n0.402356\n10.472459\n\n\n37\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\n\n1\n0.436163\n10.723995\n\n\n43\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nzarr\nicechunk\n1\n0.402587\n11.147670\n\n\n31\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\nlocal\n1\n1.001367\n38.868661\n\n\n58\ngpm_imerg\nresample\nzarr\nxesmf\nzarr\nicechunk\n1\n0.997985\n39.246454\n\n\n34\ngpm_imerg\nresample\nnetcdf\nxesmf\nzarr\nicechunk\n1\n0.999235\n46.250185\n\n\n28\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\n\n1\n1.021253\n48.669996\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"2\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n23\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n2\n0.138885\n2.382297\n\n\n14\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n2\n0.152566\n2.642978\n\n\n5\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n2\n0.152049\n2.802723\n\n\n53\ngpm_imerg\nresample\nzarr\nrioxarray\nzarr\nicechunk\n2\n0.117588\n3.109849\n\n\n50\ngpm_imerg\nresample\nzarr\npyresample\nzarr\nicechunk\n2\n0.126922\n3.223160\n\n\n17\ngpm_imerg\nresample\nnetcdf\npyresample\nzarr\nicechunk\n2\n0.161032\n3.265925\n\n\n26\ngpm_imerg\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n2\n0.158644\n3.282040\n\n\n47\ngpm_imerg\nresample\nzarr\nodc\nzarr\nicechunk\n2\n0.121783\n3.430303\n\n\n8\ngpm_imerg\nresample\nnetcdf\nodc\nzarr\nicechunk\n2\n0.160676\n3.491717\n\n\n41\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\nlocal\n2\n0.423986\n4.907868\n\n\n56\ngpm_imerg\nresample\nzarr\nsparse\nzarr\nicechunk\n2\n0.167899\n5.912529\n\n\n11\ngpm_imerg\nresample\nnetcdf\npyresample\nh5netcdf\n\n2\n0.171652\n8.141710\n\n\n2\ngpm_imerg\nresample\nnetcdf\nodc\nh5netcdf\n\n2\n0.171469\n8.328904\n\n\n20\ngpm_imerg\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n2\n0.159274\n8.705396\n\n\n62\ngpm_imerg\nresample\nzarr\nxesmfcached\nzarr\nicechunk\n2\n0.402338\n10.346199\n\n\n44\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nzarr\nicechunk\n2\n0.402595\n10.624055\n\n\n38\ngpm_imerg\nresample\nnetcdf\nxesmfcached\nh5netcdf\n\n2\n0.436163\n10.637578\n\n\n32\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\nlocal\n2\n1.001367\n38.748044\n\n\n59\ngpm_imerg\nresample\nzarr\nxesmf\nzarr\nicechunk\n2\n0.997985\n39.848156\n\n\n35\ngpm_imerg\nresample\nnetcdf\nxesmf\nzarr\nicechunk\n2\n0.999366\n42.124982\n\n\n29\ngpm_imerg\nresample\nnetcdf\nxesmf\nh5netcdf\n\n2\n1.021253\n45.680152",
    "crumbs": [
      "Profiling results",
      "Memory and time usage (GPM IMERG)"
    ]
  },
  {
    "objectID": "examples/earthdata-download.html",
    "href": "examples/earthdata-download.html",
    "title": "Download and virtualize dataset",
    "section": "",
    "text": "import os\nfrom pathlib import Path\n\nimport earthaccess\nimport rasterio\nimport rioxarray  # noqa\nimport s3fs\nimport xarray as xr\nimport zarr\nfrom common import earthaccess_args\nfrom icechunk import (\n    IcechunkStore,\n    S3Credentials,\n    StorageConfig,\n    StoreConfig,\n    VirtualRefConfig,\n)\nfrom rio_cogeo.cogeo import cog_translate\nfrom rio_cogeo.profiles import cog_profiles\nfrom virtualizarr import open_virtual_dataset\nfrom virtualizarr.writers.icechunk import dataset_to_icechunk\n\n\nSetup earthaccess query parameters\n\ndataset = \"mursst\"\ndataset_args = earthaccess_args[dataset]\nconcept_id = dataset_args[\"concept_id\"]\nfilename = dataset_args[\"filename\"]\nvariable = dataset_args[\"variable\"]\n\n\n\nAuthenticate via earthaccess\n\nearthaccess.login()\n\n\n\nDownload dataset\n\nresults = earthaccess.search_data(\n    concept_id=concept_id, count=1, temporal=(\"2002-06-01\", \"2002-06-01\")\n)\nfp = earthaccess.download(results, \"earthaccess_data\")[0]\n\n\n\nVirtualize dataset\n\ndef virtualize_dataset(local_fp):\n    \"\"\"Create a virtual reference file for a dataset\"\"\"\n\n    def local_to_s3_url(old_local_path: str) -&gt; str:\n        \"\"\"Replace local path to s3 uri for all chucks\"\"\"\n\n        new_s3_bucket_url = Path(\"/\".join(s3_uri.split(\"/\")[1:-1]))\n        filename = Path(old_local_path).name\n        new_path = f\"s3:/{str(new_s3_bucket_url / filename)}\"\n        return new_path\n\n    s3_uri = results[0].data_links(access=\"direct\")[0]\n    virtual_ds = open_virtual_dataset(str(local_fp), indexes={})\n    virtual_ds = virtual_ds.virtualize.rename_paths(local_to_s3_url)\n    virtual_ds = virtual_ds[[variable]]\n    return virtual_ds.drop_vars(\"time\")\n\n\nvirtual_ds = virtualize_dataset(fp)\n\n\n\nStore virtual dataset as kerchunk reference\n\ns3_uri = results[0].data_links(access=\"direct\")[0]\nif dataset == \"gpm_merg\":\n    output_fp = f\"earthaccess_data/{s3_uri.split('/')[-1][:-4]}.json\"\nelse:\n    output_fp = f\"earthaccess_data/{s3_uri.split('/')[-1][:-3]}.json\"\nvirtual_ds.virtualize.to_kerchunk(output_fp, format=\"json\")\n\n\n\nStore virtual dataset using icechunk\n\ns3_creds = earthaccess.get_s3_credentials(daac=dataset_args[\"daac\"])\ncredentials = S3Credentials(\n    access_key_id=s3_creds[\"accessKeyId\"],\n    secret_access_key=s3_creds[\"secretAccessKey\"],\n    session_token=s3_creds[\"sessionToken\"],\n)\nstorage = StorageConfig.s3_from_env(\n    bucket=\"nasa-veda-scratch\",\n    prefix=f\"resampling/icechunk/{dataset}-reference\",\n    region=\"us-west-2\",\n)\nconfig = StoreConfig(\n    virtual_ref_config=VirtualRefConfig.s3_from_config(credentials=credentials),\n)\nvirtual_store = IcechunkStore.open_or_create(storage=storage, config=config, mode=\"w\")\ndataset_to_icechunk(virtual_ds, virtual_store)\nvirtual_store.commit(\"Create refenence dataset\")\n\n\n\nStore dataset using Zarr V3 and icechunk\n\nvirtual_storage = StorageConfig.s3_from_env(\n    bucket=\"nasa-veda-scratch\",\n    prefix=f\"resampling/icechunk/{dataset}-reference\",\n    region=\"us-west-2\",\n)\nvirtual_store = IcechunkStore.open_existing(storage=virtual_storage, mode=\"r\")\nds = xr.open_zarr(virtual_store, zarr_format=3, consolidated=False).load()\nds = ds.drop_encoding()\nds = ds.squeeze()\nif dataset == \"gpm_imerg\":\n    ds = ds.transpose(\"lat\", \"lon\")\nencoding = {\n    variable: {\n        \"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()],\n    }\n}\nstorage = StorageConfig.s3_from_env(\n    bucket=\"nasa-veda-scratch\",\n    prefix=f\"resampling/icechunk/{dataset}\",\n    region=\"us-west-2\",\n)\nstore = IcechunkStore.open_or_create(storage=storage, mode=\"w\")\nds.to_zarr(store, zarr_format=3, consolidated=False, encoding=encoding)\nstore.commit(\"Add dataset\")\n\n\n\nStore dataset as COG\n\ndef _translate(src_path, dst_path, profile=\"zstd\", profile_options={}, **options):\n    \"\"\"\n    Convert image to COG.\n\n    From https://cogeotiff.github.io/rio-cogeo/API/\n    \"\"\"\n    # Format creation option (see gdalwarp `-co` option)\n    output_profile = cog_profiles.get(profile)\n    output_profile.update(dict(BIGTIFF=\"IF_SAFER\"))\n    output_profile.update(profile_options)\n\n    # Dataset Open option (see gdalwarp `-oo` option)\n    config = dict(\n        GDAL_NUM_THREADS=\"ALL_CPUS\",\n        GDAL_TIFF_INTERNAL_MASK=True,\n        GDAL_TIFF_OVR_BLOCKSIZE=\"128\",\n    )\n\n    cog_translate(\n        src_path,\n        dst_path,\n        output_profile,\n        config=config,\n        in_memory=False,\n        quiet=True,\n        **options,\n    )\n    return True\n\n\n# Only store MUR SST since it has the expected (time, y, x) axis order\nif dataset == \"mursst\":\n    args = earthaccess_args[dataset]\n    src = f'NETCDF:earthaccess_data/{args[\"filename\"]}:{args[\"variable\"]}'\n    dst = f\"earthaccess_data/{dataset}.tif\"\n    # Generate local COG\n    with rasterio.open(src) as da:\n        _translate(da, dst)\n    # Upload to S3\n    remote_uri = f\"s3://nasa-veda-scratch/resampling/{dataset}.tif\"\n    fs = s3fs.S3FileSystem()\n    fs.put(dst, remote_uri)\n\n\n\nStore overviews as Zarr\n\nlocal = False\nif local:\n    dst = f\"{os.getcwd()}/earthaccess_data/{dataset}-overviews.zarr\"\n    storage = StorageConfig.filesystem(dst)\n\nelse:\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-overviews.zarr\",\n        region=\"us-west-2\",\n    )\nstore = IcechunkStore.open_or_create(storage=storage, mode=\"w\")\nsrc = f\"earthaccess_data/{dataset}.tif\"\ncog = rasterio.open(src)\ndata = rioxarray.open_rasterio(src)\nscale = float(data.attrs[\"scale_factor\"])\noffset = float(data.attrs[\"add_offset\"])\ndata = rioxarray.open_rasterio(src)\ndata = data.drop_encoding()\ndata.attrs = {}\nencoding = {\"var\": {\"codecs\": [zarr.codecs.BytesCodec(), zarr.codecs.ZstdCodec()]}}\n\ndata = data.to_dataset(name=\"var\")\ndata.to_zarr(store, group=\"data\", mode=\"w\", encoding=encoding)\nfor ind, level in enumerate(cog.overviews(1)):\n    overview = rioxarray.open_rasterio(src, overview_level=ind)\n    overview = overview.load() * scale + offset\n    overview = overview.drop_encoding()\n    overview.attrs = {}\n    overview = overview.to_dataset(name=\"var\")\n    overview.to_zarr(store, group=str(level), mode=\"w\", encoding=encoding)\nstore.commit(\"Add overviews\")",
    "crumbs": [
      "Dataset preparation",
      "Download and virtualize dataset"
    ]
  },
  {
    "objectID": "examples/generate-weights.html",
    "href": "examples/generate-weights.html",
    "title": "Pre-generate weights for resampling with XESMF or sparse",
    "section": "",
    "text": "import itertools\n\nimport numpy as np\nimport pyproj\nimport rasterio.transform\nimport sparse\nimport xarray as xr\nimport xesmf as xe\nfrom common import earthaccess_args, target_extent\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef make_grid_ds(*, te, tilesize, dstSRS) -&gt; xr.Dataset:\n    \"\"\"\n    Make a dataset representing a target grid\n\n    Returns\n    -------\n    xr.Dataset\n        Target grid dataset with the following variables:\n        - \"x\": X coordinate in Web Mercator projection (grid cell center)\n        - \"y\": Y coordinate in Web Mercator projection (grid cell center)\n        - \"lat\": latitude coordinate (grid cell center)\n        - \"lon\": longitude coordinate (grid cell center)\n        - \"lat_b\": latitude bounds for grid cell\n        - \"lon_b\": longitude bounds for grid cell\n\n    Notes\n    -----\n    Modified from ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    transform = rasterio.transform.Affine.translation(\n        te[0], te[3]\n    ) * rasterio.transform.Affine.scale((te[2] * 2) / tilesize, (te[1] * 2) / tilesize)\n\n    p = pyproj.Proj(dstSRS)\n\n    grid_shape = (tilesize, tilesize)\n    bounds_shape = (tilesize + 1, tilesize + 1)\n\n    xs = np.empty(grid_shape)\n    ys = np.empty(grid_shape)\n    lat = np.empty(grid_shape)\n    lon = np.empty(grid_shape)\n    lat_b = np.zeros(bounds_shape)\n    lon_b = np.zeros(bounds_shape)\n\n    # calc grid cell center coordinates\n    ii, jj = np.meshgrid(np.arange(tilesize) + 0.5, np.arange(tilesize) + 0.5)\n    for i, j in itertools.product(range(grid_shape[0]), range(grid_shape[1])):\n        locs = [ii[i, j], jj[i, j]]\n        xs[i, j], ys[i, j] = transform * locs\n        lon[i, j], lat[i, j] = p(xs[i, j], ys[i, j], inverse=True)\n\n    # calc grid cell bounds\n    iib, jjb = np.meshgrid(np.arange(tilesize + 1), np.arange(tilesize + 1))\n    for i, j in itertools.product(range(bounds_shape[0]), range(bounds_shape[1])):\n        locs = [iib[i, j], jjb[i, j]]\n        x, y = transform * locs\n        lon_b[i, j], lat_b[i, j] = p(x, y, inverse=True)\n\n    latitude = xr.DataArray(\n        lat[:, 0],\n        dims=\"y\",\n        attrs=dict(\n            standard_name=\"latitude\",\n            long_name=\"Latitude\",\n            units=\"degrees_north\",\n            axis=\"X\",\n        ),\n    )\n    longitude = xr.DataArray(\n        lon[0, :],\n        dims=\"x\",\n        attrs=dict(\n            standard_name=\"longitude\",\n            long_name=\"Longitude\",\n            units=\"degrees_east\",\n            axis=\"Y\",\n        ),\n    )\n\n    return xr.Dataset(\n        {\n            \"lat_b\": xr.DataArray(lat_b, dims=[\"y_b\", \"x_b\"]),\n            \"lon_b\": xr.DataArray(lon_b, dims=[\"y_b\", \"x_b\"]),\n        },\n        {\n            \"latitude\": latitude,\n            \"longitude\": longitude,\n        },\n    )\n\n\ndef xesmf_weights_to_xarray(regridder) -&gt; xr.Dataset:\n    \"\"\"\n    Construct an xarray dataset from XESMF weights\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    w = regridder.weights.data\n    dim = \"n_s\"\n    ds = xr.Dataset(\n        {\n            \"S\": (dim, w.data),\n            \"col\": (dim, w.coords[1, :] + 1),\n            \"row\": (dim, w.coords[0, :] + 1),\n        }\n    )\n    ds.attrs = {\"n_in\": regridder.n_in, \"n_out\": regridder.n_out}\n    return ds\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef generate_weights(dataset, zoom):\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Create icechunk repos for caching weights and target grid\n    weights_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-weights-{zoom}\",\n        region=\"us-west-2\",\n    )\n    target_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-target-{zoom}\",\n        region=\"us-west-2\",\n    )\n    weights_store = IcechunkStore.open_or_create(storage=weights_storage, mode=\"w\")\n    target_store = IcechunkStore.open_or_create(storage=target_storage, mode=\"w\")\n    # Create target grid\n    target_grid = make_grid_ds(te=te, tilesize=256, dstSRS=\"EPSG:3857\")\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Chunk target grid for parallel weights generations\n    output_chunk_size = 128\n    target_grid = target_grid.chunk(\n        {\n            \"x\": output_chunk_size,\n            \"y\": output_chunk_size,\n            \"y_b\": output_chunk_size,\n            \"x_b\": output_chunk_size,\n        }\n    )\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        target_grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n        parallel=True,\n    )\n    # Convert weigts to a dataset\n    weights = xesmf_weights_to_xarray(regridder)\n    # Store weights using icechunk\n    weights.to_zarr(weights_store, zarr_format=3, consolidated=False)\n    # Commit data to icechunk stores\n    weights_store.commit(\"Store weights\")\n    # Store target grid using icechunk\n    target_grid.load().to_zarr(target_store, zarr_format=3, consolidated=False)\n    target_store.commit(\"Generate target grid\")\n    # Store weights using Zarr\n    output = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-weights-{zoom}.zarr\"\n    weights.to_zarr(output, mode=\"w\", storage_options={\"use_listings_cache\": False})\n    # Store target grid using Zarr\n    output = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-target-{zoom}.zarr\"\n    target_grid.to_zarr(output, mode=\"w\", storage_options={\"use_listings_cache\": False})\n\n\ndataset = \"gpm_imerg\"\ngenerate_weights(dataset, 0)\n\n\ngenerate_weights(dataset, 1)\n\n\ngenerate_weights(dataset, 2)",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "Pre-generate weights"
    ]
  },
  {
    "objectID": "examples/process-results.html",
    "href": "examples/process-results.html",
    "title": "Process results",
    "section": "",
    "text": "import hvplot.pandas  # noqa\nfrom utils import process_results\ndf = process_results(\"results/2024-11-01\").sort_values([\"duration (s)\"])",
    "crumbs": [
      "Profiling results",
      "Memory and time usage (MURSST)"
    ]
  },
  {
    "objectID": "examples/process-results.html#show-memory-and-time-for-warp-resampling-dataset",
    "href": "examples/process-results.html#show-memory-and-time-for-warp-resampling-dataset",
    "title": "Process results",
    "section": "Show memory and time for warp resampling dataset",
    "text": "Show memory and time for warp resampling dataset\n\ndf = df[df[\"task\"] == \"resample\"]\ndf = df[df[\"dataset\"] == \"mursst\"]\ndf[df[\"zoom\"] == \"0\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n7\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n0\n0.030880\n0.451393\n\n\n0\nmursst\nresample\ncog\nrasterio\ncog\n\n0\n0.039943\n1.096481\n\n\n91\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n0\n0.090285\n1.296510\n\n\n98\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n0\n0.177909\n3.768114\n\n\n84\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n0\n10.000827\n16.922882\n\n\n119\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n0\n10.004556\n20.016685\n\n\n77\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n0\n11.744682\n24.109180\n\n\n49\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n0\n39.145886\n24.734611\n\n\n28\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n0\n10.532483\n25.527201\n\n\n42\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n0\n39.148321\n26.894815\n\n\n112\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n0\n39.148897\n27.055192\n\n\n105\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n0\n10.538741\n28.138383\n\n\n21\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n0\n10.533193\n29.754768\n\n\n56\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n0\n8.213028\n35.846871\n\n\n35\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n0\n39.166931\n43.924830\n\n\n70\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n0\n11.766091\n45.325058\n\n\n14\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n0\n10.553407\n45.466367\n\n\n63\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n0\n8.591809\n129.674675\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"1\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n8\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n1\n0.032527\n0.457462\n\n\n1\nmursst\nresample\ncog\nrasterio\ncog\n\n1\n0.043156\n1.018797\n\n\n92\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n1\n0.229896\n1.568192\n\n\n99\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n1\n0.177740\n3.652056\n\n\n85\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n1\n2.623672\n6.069850\n\n\n120\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n1\n2.624567\n6.697549\n\n\n78\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n1\n3.017012\n7.185708\n\n\n29\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n1\n2.723991\n8.505732\n\n\n57\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n1\n2.508900\n8.554535\n\n\n22\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n1\n2.725570\n8.711783\n\n\n50\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n1\n9.843777\n9.053228\n\n\n43\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n1\n9.845207\n9.261657\n\n\n106\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n1\n2.724917\n9.289789\n\n\n113\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n1\n9.843665\n9.592978\n\n\n71\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n1\n3.039601\n15.258607\n\n\n15\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n1\n2.744846\n17.696822\n\n\n36\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n1\n9.865529\n17.706269\n\n\n64\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n1\n2.887681\n42.658372\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"2\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n9\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n2\n0.033333\n0.458129\n\n\n2\nmursst\nresample\ncog\nrasterio\ncog\n\n2\n0.041837\n1.082959\n\n\n58\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n2\n0.457986\n2.178969\n\n\n93\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n2\n0.711039\n2.785214\n\n\n79\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n2\n0.426108\n3.197895\n\n\n100\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n2\n0.144734\n3.503289\n\n\n121\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n2\n0.433782\n3.655337\n\n\n86\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n2\n0.432031\n3.731052\n\n\n23\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n2\n0.439379\n3.760564\n\n\n107\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n2\n0.437503\n4.148531\n\n\n30\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n2\n0.436650\n4.165913\n\n\n44\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n2\n1.433250\n4.229222\n\n\n51\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n2\n1.419420\n4.454776\n\n\n114\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n2\n1.183613\n4.589174\n\n\n72\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n2\n0.447514\n8.683591\n\n\n16\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n2\n0.458493\n9.655560\n\n\n65\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n2\n0.823933\n10.252551\n\n\n37\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n2\n1.553817\n11.060682\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"3\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n10\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n3\n0.036760\n0.461121\n\n\n3\nmursst\nresample\ncog\nrasterio\ncog\n\n3\n0.045020\n1.139604\n\n\n59\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n3\n0.177466\n1.456922\n\n\n80\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n3\n0.173336\n2.767960\n\n\n24\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n3\n0.189597\n3.242828\n\n\n122\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n3\n0.182754\n3.287474\n\n\n45\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n3\n0.314083\n3.315556\n\n\n101\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n3\n0.134824\n3.372671\n\n\n87\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n3\n0.182664\n3.444906\n\n\n108\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n3\n0.187025\n3.598609\n\n\n31\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n3\n0.187224\n3.624480\n\n\n52\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n3\n0.413825\n3.651580\n\n\n115\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n3\n0.532895\n3.705541\n\n\n94\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n3\n2.665296\n5.434425\n\n\n66\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n3\n0.540422\n7.526660\n\n\n73\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n3\n0.194413\n8.629072\n\n\n38\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n3\n0.308219\n9.300283\n\n\n17\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n3\n0.209587\n9.871493\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"4\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n11\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n4\n0.043987\n0.468173\n\n\n95\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n4\n0.031694\n0.966727\n\n\n4\nmursst\nresample\ncog\nrasterio\ncog\n\n4\n0.052212\n1.053465\n\n\n60\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n4\n0.111228\n1.276880\n\n\n81\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n4\n0.122476\n2.702871\n\n\n25\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n4\n0.139749\n3.131053\n\n\n46\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n4\n0.256487\n3.180913\n\n\n123\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n4\n0.143009\n3.402756\n\n\n102\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n4\n0.118861\n3.418559\n\n\n88\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n4\n0.141879\n3.462004\n\n\n109\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n4\n0.145801\n3.509928\n\n\n32\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n4\n0.147414\n3.568310\n\n\n53\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n4\n0.253066\n3.669410\n\n\n116\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n4\n0.253287\n3.686281\n\n\n67\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n4\n0.477397\n6.907007\n\n\n74\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n4\n0.143667\n8.635026\n\n\n39\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n4\n0.275444\n8.986320\n\n\n18\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n4\n0.158676\n9.107896\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"5\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n12\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n5\n0.105439\n0.515439\n\n\n96\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n5\n0.039705\n1.012773\n\n\n5\nmursst\nresample\ncog\nrasterio\ncog\n\n5\n0.113595\n1.112734\n\n\n61\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n5\n0.088913\n1.215079\n\n\n82\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n5\n0.121621\n2.654118\n\n\n26\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n5\n0.135427\n3.033522\n\n\n47\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n5\n0.253184\n3.210790\n\n\n124\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n5\n0.129291\n3.252786\n\n\n103\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n5\n0.118843\n3.367720\n\n\n33\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n5\n0.129850\n3.405619\n\n\n89\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n5\n0.128072\n3.412353\n\n\n110\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n5\n0.131152\n3.436898\n\n\n54\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n5\n0.249761\n3.550618\n\n\n117\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n5\n0.251038\n3.593822\n\n\n68\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n5\n0.463590\n6.945700\n\n\n75\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n5\n0.142686\n7.491875\n\n\n19\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n5\n0.155406\n8.900083\n\n\n40\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n5\n0.272323\n8.948425\n\n\n\n\n\n\ndf[df[\"zoom\"] == \"6\"].style.background_gradient(cmap=\"YlOrRd\")\n\n\n\n\n\n\n \ndataset\ntask\nformat\nmethod\ndriver\nvirtual\nzoom\npeak memory (GB)\nduration (s)\n\n\n\n\n13\nmursst\nresample\ncog\nrasterio\ncog\nlocal\n6\n0.104144\n0.499831\n\n\n97\nmursst\nresample\nweboptimizedzarr\nrasterio\nzarr\nicechunk\n6\n0.039679\n1.003966\n\n\n6\nmursst\nresample\ncog\nrasterio\ncog\n\n6\n0.112608\n1.114690\n\n\n62\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nlocal\n6\n0.088630\n1.209784\n\n\n83\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\nlocal\n6\n0.121559\n2.723510\n\n\n27\nmursst\nresample\nnetcdf\nodc\nh5netcdf\nlocal\n6\n0.135140\n3.024937\n\n\n48\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\nlocal\n6\n0.251392\n3.235234\n\n\n90\nmursst\nresample\nnetcdf\nrioxarray\nzarr\nicechunk\n6\n0.127879\n3.332770\n\n\n104\nmursst\nresample\nweboptimizedzarr\nrioxarray\nzarr\nicechunk\n6\n0.118704\n3.334266\n\n\n125\nmursst\nresample\nzarr\nrioxarray\nzarr\nicechunk\n6\n0.123344\n3.345806\n\n\n111\nmursst\nresample\nzarr\nodc\nzarr\nicechunk\n6\n0.125025\n3.362655\n\n\n34\nmursst\nresample\nnetcdf\nodc\nzarr\nicechunk\n6\n0.129562\n3.370525\n\n\n118\nmursst\nresample\nzarr\npyresample\nzarr\nicechunk\n6\n0.249231\n3.624967\n\n\n55\nmursst\nresample\nnetcdf\npyresample\nzarr\nicechunk\n6\n0.249000\n3.675096\n\n\n69\nmursst\nresample\nnetcdf\nrasterio\nnetcdf\nvsis3\n6\n0.463311\n7.042069\n\n\n76\nmursst\nresample\nnetcdf\nrioxarray\nh5netcdf\n\n6\n0.142624\n8.404518\n\n\n20\nmursst\nresample\nnetcdf\nodc\nh5netcdf\n\n6\n0.154987\n8.803269\n\n\n41\nmursst\nresample\nnetcdf\npyresample\nh5netcdf\n\n6\n0.271413\n9.045595",
    "crumbs": [
      "Profiling results",
      "Memory and time usage (MURSST)"
    ]
  },
  {
    "objectID": "examples/resample-cog-rasterio-cog-local.html",
    "href": "examples/resample-cog-rasterio-cog-local.html",
    "title": "Resampling with rasterio (Local storage, COG file, COG driver)",
    "section": "",
    "text": "import argparse\n\nimport numpy as np\nimport rasterio\nfrom rasterio.vrt import WarpedVRT\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import target_extent\n\n    te = target_extent[zoom]\n\n    # Define source and target projection\n    srcSRS = \"EPSG:4326\"\n    dstSRS = \"EPSG:3857\"\n    width = height = 256\n\n    src = f\"earthaccess_data/{dataset}.tif\"\n\n    with rasterio.open(src) as da:\n        with WarpedVRT(da, src_crs=srcSRS, crs=dstSRS) as vrt:\n\n            dst_window = vrt.window(*te)\n\n            data = vrt.read(window=dst_window, out_shape=(height, width), masked=True)\n            # Mask and fill array\n            data = data.astype(\"float32\", casting=\"unsafe\")\n            np.multiply(data, da.scales[0], out=data, casting=\"unsafe\")\n            np.add(data, da.offsets[0], out=data, casting=\"unsafe\")\n            return data\n\n\n%%time\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))"
  },
  {
    "objectID": "examples/resample-netcdf-odc-h5netcdf-local.html",
    "href": "examples/resample-netcdf-odc-h5netcdf-local.html",
    "title": "Resampling with ODC-geo (local storage, NetCDF File, H5NetCDF driver)",
    "section": "",
    "text": "import argparse\n\nimport fsspec\nimport xarray as xr\nfrom odc.geo.geobox import GeoBox\nfrom odc.geo.geom import Geometry\nfrom odc.geo.xr import crop, xr_reproject\nfrom shapely.geometry import box\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    src = f'earthaccess_data/{args[\"filename\"]}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    fs = fsspec.filesystem(\"file\")\n    with fs.open(src, **fsspec_caching) as f:\n        # Define ODC geobox for target tile\n        gbox = GeoBox.from_bbox(te, dstSRS, shape=(height, width))\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={})[args[\"variable\"]]\n        if dataset == \"gpm_imerg\":\n            # Transpose and rename dataset dims to align with GDAL expectations\n            da = (\n                da.rename({\"lon\": \"x\", \"lat\": \"y\"})\n                .transpose(\"time\", \"y\", \"x\")\n                .squeeze()\n            )\n        # Assign input projection\n        da = da.odc.assign_crs(srcSRS)\n        # Crop dataset to tile bounds\n        bbox = box(*te)\n        geom = Geometry(bbox, \"EPSG:3857\")\n        da = crop(da, geom)\n        # Load into memory to avoid topology error\n        da.load()\n        # Reproject dataset\n        return xr_reproject(da, gbox).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Open Data Cube",
      "H5NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-pyresample-h5netcdf-.html",
    "href": "examples/resample-netcdf-pyresample-h5netcdf-.html",
    "title": "Resampling with pyresample (s3 storage, NetCDF File, H5NetCDF driver, earthaccess auth)",
    "section": "",
    "text": "import argparse\n\nimport earthaccess\nimport xarray as xr\nfrom pyresample.area_config import create_area_def\nfrom pyresample.gradient import block_nn_interpolator, gradient_resampler_indices_block\nfrom pyresample.resampler import resample_blocks\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    input_uri = f'{args[\"folder\"]}/{args[\"filename\"]}'\n    src = f's3://{args[\"bucket\"]}/{input_uri}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Authentical via earthaccess\n    earthaccess.login()\n    fs = earthaccess.get_s3_filesystem(daac=args[\"daac\"])\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={})[args[\"variable\"]]\n        # Rechunk MURSST to operate on fewer chunks\n        if dataset == \"mursst\":\n            da = da.chunk({\"time\": -1, \"lat\": 4000, \"lon\": 4000})\n        elif dataset == \"gpm_imerg\":\n            # Transpose dims to align with pyresample expectations\n            da = da.transpose(\"time\", \"lat\", \"lon\").squeeze()\n        # Create area definition for the target dataset\n        target_area_def = create_area_def(\n            area_id=1,\n            projection=dstSRS,\n            shape=(height, width),\n            area_extent=te,\n        )\n        # Create area definition for the source dataset\n        source_area_def = create_area_def(\n            area_id=2,\n            projection=srcSRS,\n            shape=(da.sizes[\"lat\"], da.sizes[\"lon\"]),\n            area_extent=[-179.995, 89.995, 180.005, -89.995],\n        )\n        # Compute indices for resampling\n        indices_xy = resample_blocks(\n            gradient_resampler_indices_block,\n            source_area_def,\n            [],\n            target_area_def,\n            chunk_size=(1, height, width),\n            dtype=float,\n        )\n        # Apply resampler\n        resampled = resample_blocks(\n            block_nn_interpolator,\n            source_area_def,\n            [da.data],\n            target_area_def,\n            dst_arrays=[indices_xy],\n            chunk_size=(1, height, width),\n            dtype=da.dtype,\n        )\n        # Reproject dataset\n        return resampled.compute()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Pyresample",
      "H5NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-pyresample-zarr-icechunk.html",
    "href": "examples/resample-netcdf-pyresample-zarr-icechunk.html",
    "title": "Resampling with pyresample (S3 storage, NetCDF file, Zarr reader, icechunk virtualization)",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom pyresample.area_config import create_area_def\nfrom pyresample.gradient import block_nn_interpolator, gradient_resampler_indices_block\nfrom pyresample.resampler import resample_blocks\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-reference\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Rechunk MURSST to operate on fewer chunks\n    if dataset == \"mursst\":\n        da = da.chunk({\"time\": -1, \"lat\": 4000, \"lon\": 4000})\n    elif dataset == \"gpm_imerg\":\n        # Transpose dims to align with pyresample expectations\n        da = da.transpose(\"time\", \"lat\", \"lon\").squeeze()\n    # Create area definition for the target dataset\n    target_area_def = create_area_def(\n        area_id=1,\n        projection=dstSRS,\n        shape=(height, width),\n        area_extent=te,\n    )\n    # Create area definition for the source dataset\n    source_area_def = create_area_def(\n        area_id=2,\n        projection=srcSRS,\n        shape=(da.sizes[\"lat\"], da.sizes[\"lon\"]),\n        area_extent=[-179.995, 89.995, 180.005, -89.995],\n    )\n    # Compute indices for resampling\n    indices_xy = resample_blocks(\n        gradient_resampler_indices_block,\n        source_area_def,\n        [],\n        target_area_def,\n        chunk_size=(1, height, width),\n        dtype=float,\n    )\n    # Apply resampler\n    resampled = resample_blocks(\n        block_nn_interpolator,\n        source_area_def,\n        [da.data],\n        target_area_def,\n        dst_arrays=[indices_xy],\n        chunk_size=(1, height, width),\n        dtype=da.dtype,\n    )\n    # Reproject dataset\n    return resampled.compute()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Pyresample",
      "Zarr Reader + Icechunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-rasterio-netcdf-vsis3.html",
    "href": "examples/resample-netcdf-rasterio-netcdf-vsis3.html",
    "title": "Resampling with rasterio (S3 storage, NetCDF File, NetCDF4 driver, VSIS3 virtualization, earthaccess auth)",
    "section": "",
    "text": "import argparse\nimport json\n\nimport geopandas as gpd\nimport numpy as np\nimport rasterio\nfrom rasterio.mask import mask\nfrom rasterio.session import AWSSession\nfrom rasterio.warp import reproject\nfrom shapely.geometry import box\n\n\ndef configure_auth(daac):\n    \"\"\"Authenticate using earthaccess\"\"\"\n    import boto3\n    import earthaccess\n\n    auth = earthaccess.login()\n    s3_credentials = auth.get_s3_credentials(daac)\n    session = boto3.Session(\n        aws_access_key_id=s3_credentials[\"accessKeyId\"],\n        aws_secret_access_key=s3_credentials[\"secretAccessKey\"],\n        aws_session_token=s3_credentials[\"sessionToken\"],\n        region_name=\"us-west-2\",\n    )\n    rio_env = rasterio.Env(\n        AWSSession(session),\n    )\n    rio_env.__enter__()\n    return rio_env\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    src = f'NETCDF:/vsis3/{args[\"bucket\"]}/{args[\"folder\"]}/{args[\"filename\"]}:{args[\"variable\"]}'\n    # Authenticate with earthaccess\n    with configure_auth(args[\"daac\"]):\n        # Define source and target projection\n        dstSRS = \"EPSG:3857\"\n        srcSRS = \"EPSG:4326\"\n        width = height = 256\n        with rasterio.open(src) as da:\n            # Clip dataset to bounds of Web Mercator tile\n            bbox = box(*te)\n            geo = gpd.GeoDataFrame(\n                {\"geometry\": bbox}, index=[0], crs=int(dstSRS.split(\":\")[1])\n            )\n            geo = geo.to_crs(crs=srcSRS)\n            coords = [json.loads(geo.to_json())[\"features\"][0][\"geometry\"]]\n            arr, src_transform = mask(da, shapes=coords, crop=True)\n            # Mask and fill array\n            ma = arr.astype(\"float32\", casting=\"unsafe\")\n            np.multiply(ma, da.scales[0], out=ma, casting=\"unsafe\")\n            np.add(ma, da.offsets[0], out=ma, casting=\"unsafe\")\n            # Define affine transformation from input to output dataset\n            dst_transform = rasterio.transform.from_bounds(*te, width, height)\n            # Create array to host results\n            destination = np.zeros((height, width), np.float32)\n            # Reproject dataset\n            _, transform = reproject(\n                ma.squeeze(),\n                destination,\n                src_crs=srcSRS,\n                src_transform=src_transform,\n                dst_crs=dstSRS,\n                dst_transform=dst_transform,\n            )\n            return destination\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rasterio",
      "NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-rioxarray-h5netcdf-local.html",
    "href": "examples/resample-netcdf-rioxarray-h5netcdf-local.html",
    "title": "Resampling with Rioxarray (Local storage, NetCDF file, H5NetCDF driver)",
    "section": "",
    "text": "import argparse\n\nimport fsspec\nimport rasterio as rio  # noqa\nimport xarray as xr\nfrom rasterio.warp import calculate_default_transform\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    src = f'earthaccess_data/{args[\"filename\"]}'\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    fs = fsspec.filesystem(\"file\")\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        if dataset == \"gpm_imerg\":\n            # Transpose and rename dims to align with rioxarray expectations\n            da = da.rename({\"lon\": \"x\", \"lat\": \"y\"}).transpose(\"time\", \"y\", \"x\")\n        # Set input dataset projection\n        da = da.rio.write_crs(srcSRS)\n        da = da.rio.clip_box(\n            *te,\n            crs=dstSRS,\n        )\n        # Define affine transformation from input to output dataset\n        dst_transform, w, h = calculate_default_transform(\n            srcSRS,\n            dstSRS,\n            da.rio.width,\n            da.rio.height,\n            *da.rio.bounds(),\n            dst_width=width,\n            dst_height=height,\n        )\n        # Reproject dataset\n        return da.rio.reproject(dstSRS, shape=(h, w), transform=dst_transform)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Rioxarray",
      "H5NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmf-h5netcdf-.html",
    "href": "examples/resample-netcdf-xesmf-h5netcdf-.html",
    "title": "Resampling with XESMF (S3 storage, NetCDF file, H5NetCDF driver, earthaccess auth)",
    "section": "",
    "text": "import argparse\nimport itertools\n\nimport earthaccess\nimport numpy as np\nimport pyproj\nimport rasterio.transform\nimport xarray as xr\nimport xesmf as xe\n\n\ndef make_grid_ds(*, te, tilesize, dstSRS) -&gt; xr.Dataset:\n    \"\"\"\n    Make a dataset representing a target grid\n\n    Returns\n    -------\n    xr.Dataset\n        Target grid dataset with the following variables:\n        - \"x\": X coordinate in Web Mercator projection (grid cell center)\n        - \"y\": Y coordinate in Web Mercator projection (grid cell center)\n        - \"lat\": latitude coordinate (grid cell center)\n        - \"lon\": longitude coordinate (grid cell center)\n        - \"lat_b\": latitude bounds for grid cell\n        - \"lon_b\": longitude bounds for grid cell\n\n    Notes\n    -----\n    Modified from ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    transform = rasterio.transform.Affine.translation(\n        te[0], te[3]\n    ) * rasterio.transform.Affine.scale((te[2] * 2) / tilesize, (te[1] * 2) / tilesize)\n\n    p = pyproj.Proj(dstSRS)\n\n    grid_shape = (tilesize, tilesize)\n    bounds_shape = (tilesize + 1, tilesize + 1)\n\n    xs = np.empty(grid_shape)\n    ys = np.empty(grid_shape)\n    lat = np.empty(grid_shape)\n    lon = np.empty(grid_shape)\n    lat_b = np.zeros(bounds_shape)\n    lon_b = np.zeros(bounds_shape)\n\n    # calc grid cell center coordinates\n    ii, jj = np.meshgrid(np.arange(tilesize) + 0.5, np.arange(tilesize) + 0.5)\n    for i, j in itertools.product(range(grid_shape[0]), range(grid_shape[1])):\n        locs = [ii[i, j], jj[i, j]]\n        xs[i, j], ys[i, j] = transform * locs\n        lon[i, j], lat[i, j] = p(xs[i, j], ys[i, j], inverse=True)\n\n    # calc grid cell bounds\n    iib, jjb = np.meshgrid(np.arange(tilesize + 1), np.arange(tilesize + 1))\n    for i, j in itertools.product(range(bounds_shape[0]), range(bounds_shape[1])):\n        locs = [iib[i, j], jjb[i, j]]\n        x, y = transform * locs\n        lon_b[i, j], lat_b[i, j] = p(x, y, inverse=True)\n\n    return xr.Dataset(\n        {\n            \"x\": xr.DataArray(xs[0, :], dims=[\"x\"]),\n            \"y\": xr.DataArray(ys[:, 0], dims=[\"y\"]),\n            \"lat\": xr.DataArray(lat, dims=[\"y\", \"x\"]),\n            \"lon\": xr.DataArray(lon, dims=[\"y\", \"x\"]),\n            \"lat_b\": xr.DataArray(lat_b, dims=[\"y_b\", \"x_b\"]),\n            \"lon_b\": xr.DataArray(lon_b, dims=[\"y_b\", \"x_b\"]),\n        },\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    input_uri = f'{args[\"folder\"]}/{args[\"filename\"]}'\n    src = f's3://{args[\"bucket\"]}/{input_uri}'\n    # Create grid to hold result\n    target_grid = make_grid_ds(te=te, tilesize=256, dstSRS=\"EPSG:3857\")\n    # Authenticate with earthaccess\n    fs = earthaccess.get_s3_filesystem(daac=args[\"daac\"])\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", chunks={}, mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        # Create XESMF regridder\n        regridder = xe.Regridder(\n            da,\n            target_grid,\n            \"nearest_s2d\",\n            periodic=True,\n            extrap_method=\"nearest_s2d\",\n            ignore_degenerate=True,\n        )\n        # Regrid dataset\n        return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF",
      "H5NetCDF Driver (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmf-zarr-icechunk.html",
    "href": "examples/resample-netcdf-xesmf-zarr-icechunk.html",
    "title": "Resampling with XESMF (S3 storage, NetCDF file, Zarr reader, icechunk virtualization)",
    "section": "",
    "text": "import argparse\nimport itertools\n\nimport numpy as np\nimport pyproj\nimport rasterio.transform\nimport xarray as xr\nimport xesmf as xe\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef make_grid_ds(*, te, tilesize, dstSRS) -&gt; xr.Dataset:\n    \"\"\"\n    Make a dataset representing a target grid\n\n    Returns\n    -------\n    xr.Dataset\n        Target grid dataset with the following variables:\n        - \"x\": X coordinate in Web Mercator projection (grid cell center)\n        - \"y\": Y coordinate in Web Mercator projection (grid cell center)\n        - \"lat\": latitude coordinate (grid cell center)\n        - \"lon\": longitude coordinate (grid cell center)\n        - \"lat_b\": latitude bounds for grid cell\n        - \"lon_b\": longitude bounds for grid cell\n\n    Notes\n    -----\n    Modified from ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n\n    transform = rasterio.transform.Affine.translation(\n        te[0], te[3]\n    ) * rasterio.transform.Affine.scale((te[2] * 2) / tilesize, (te[1] * 2) / tilesize)\n\n    p = pyproj.Proj(dstSRS)\n\n    grid_shape = (tilesize, tilesize)\n    bounds_shape = (tilesize + 1, tilesize + 1)\n\n    xs = np.empty(grid_shape)\n    ys = np.empty(grid_shape)\n    lat = np.empty(grid_shape)\n    lon = np.empty(grid_shape)\n    lat_b = np.zeros(bounds_shape)\n    lon_b = np.zeros(bounds_shape)\n\n    # calc grid cell center coordinates\n    ii, jj = np.meshgrid(np.arange(tilesize) + 0.5, np.arange(tilesize) + 0.5)\n    for i, j in itertools.product(range(grid_shape[0]), range(grid_shape[1])):\n        locs = [ii[i, j], jj[i, j]]\n        xs[i, j], ys[i, j] = transform * locs\n        lon[i, j], lat[i, j] = p(xs[i, j], ys[i, j], inverse=True)\n\n    # calc grid cell bounds\n    iib, jjb = np.meshgrid(np.arange(tilesize + 1), np.arange(tilesize + 1))\n    for i, j in itertools.product(range(bounds_shape[0]), range(bounds_shape[1])):\n        locs = [iib[i, j], jjb[i, j]]\n        x, y = transform * locs\n        lon_b[i, j], lat_b[i, j] = p(x, y, inverse=True)\n\n    return xr.Dataset(\n        {\n            \"x\": xr.DataArray(xs[0, :], dims=[\"x\"]),\n            \"y\": xr.DataArray(ys[:, 0], dims=[\"y\"]),\n            \"lat\": xr.DataArray(lat, dims=[\"y\", \"x\"]),\n            \"lon\": xr.DataArray(lon, dims=[\"y\", \"x\"]),\n            \"lat_b\": xr.DataArray(lat_b, dims=[\"y_b\", \"x_b\"]),\n            \"lon_b\": xr.DataArray(lon_b, dims=[\"y_b\", \"x_b\"]),\n        },\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Create grid to hold result\n    target_grid = make_grid_ds(te=te, tilesize=256, dstSRS=\"EPSG:3857\")\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-reference\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        target_grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n    )\n    # Regrid dataset\n    return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF",
      "Zarr Reader + Icechunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmfcached-h5netcdf-local.html",
    "href": "examples/resample-netcdf-xesmfcached-h5netcdf-local.html",
    "title": "Resampling with XESMF (Local storage, NetCDF file, H5NetCDF driver, and pre-generated weights)",
    "section": "",
    "text": "import argparse\n\nimport fsspec\nimport numpy as np\nimport xarray as xr\nimport xesmf as xe\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef reconstruct_weights(weights_fp):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    return _reconstruct_xesmf_weights(xr.open_zarr(weights_fp))\n\n\ndef regrid(dataset, zoom=0):\n\n    from common import earthaccess_args  # noqa: 402\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-weights-{zoom}.zarr\"\n    target_grid_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-target-{zoom}.zarr\"\n    weights = reconstruct_weights(weights_fp)\n    grid = xr.open_zarr(target_grid_fp)\n    # Define filepath, driver, and variable information\n    src = f'earthaccess_data/{args[\"filename\"]}'\n    fs = fsspec.filesystem(\"file\")\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    with fs.open(src, **fsspec_caching) as f:\n        # Open dataset\n        da = xr.open_dataset(f, engine=\"h5netcdf\", mask_and_scale=True)[\n            args[\"variable\"]\n        ]\n        # Create XESMF regridder\n        regridder = xe.Regridder(\n            da,\n            grid,\n            \"nearest_s2d\",\n            periodic=True,\n            extrap_method=\"nearest_s2d\",\n            ignore_degenerate=True,\n            reuse_weights=True,\n            weights=weights,\n        )\n        # Regrid dataset\n        return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "H5NetCDF Driver (Local storage)"
    ]
  },
  {
    "objectID": "examples/resample-netcdf-xesmfcached-zarr-kerchunk.html",
    "href": "examples/resample-netcdf-xesmfcached-zarr-kerchunk.html",
    "title": "Resampling with XESMF (S3 storage, NetCDF file, Zarr reader, kerchunk virtualization, and pre-generated weights)",
    "section": "",
    "text": "import argparse\n\nimport earthaccess\nimport fsspec\nimport numpy as np\nimport xarray as xr\nimport xesmf as xe\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef reconstruct_weights(weights_fp):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    return _reconstruct_xesmf_weights(xr.open_zarr(weights_fp))\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args  # noqa: 402\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-weights-{zoom}.zarr\"\n    target_grid_fp = f\"s3://nasa-veda-scratch/resampling/test-weight-caching/{dataset}-target-{zoom}.zarr\"\n    weights = reconstruct_weights(weights_fp)\n    grid = xr.open_zarr(target_grid_fp)\n    if dataset == \"gpm_imerg\":\n        src = f'earthaccess_data/{args[\"filename\"][:-4]}.json'\n    else:\n        src = f'earthaccess_data/{args[\"filename\"][:-3]}.json'\n    # Authenticate with earthaccess\n    s3_fs = earthaccess.get_s3fs_session(daac=args[\"daac\"])\n    storage_options = s3_fs.storage_options.copy()\n    # Specify fsspec caching since default options don't work well for raster data\n    fsspec_caching = {\n        \"cache_type\": \"none\",\n    }\n    # Open dataset using kerchunk\n    fs = fsspec.filesystem(\"reference\", fo=src, **fsspec_caching)\n    m = fs.get_mapper(\"\")\n    da = xr.open_dataset(\n        m,\n        engine=\"kerchunk\",\n        chunks={},\n        storage_options=storage_options,\n    )[args[\"variable\"]]\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n        reuse_weights=True,\n        weights=weights,\n    )\n    # Regrid dataset\n    return regridder(da).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "Zarr Reader + Kerchunk virtualization (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-weboptimizedzarr-rioxarray-zarr-icechunk.html",
    "href": "examples/resample-weboptimizedzarr-rioxarray-zarr-icechunk.html",
    "title": "Resampling with Rioxarray (S3 storage, Web-Optimized Zarr V3 store, Zarr reader with icechunk)",
    "section": "",
    "text": "WARNING: This notebook is intented to show the potential for web-optimized Zarrs that contain overviews, but the approach should not be used in production due to the lack a robust approach following a metadata specification.\n\nimport argparse\n\nfrom icechunk import IcechunkStore, StorageConfig\nfrom rasterio.crs import CRS\nfrom rasterio.transform import from_bounds\nfrom rasterio.warp import calculate_default_transform\nfrom xarray import open_zarr\n\n\ndef get_overview_level(\n    dataset_bounds,\n    dataset_shape,\n    target_bounds: tuple,\n    overviews: list,\n    height: int = 256,\n    width: int = 256,\n    srcSRS: CRS = CRS.from_string(\"EPSG:4326\"),\n    dstSRS: CRS = CRS.from_string(\"EPSG:3857\"),\n) -&gt; int:\n    \"\"\"Return the overview level corresponding to the tile resolution.\n\n    Freely adapted from rio-tiler, which freely adapted from https://github.com/OSGeo/gdal/blob/41993f127e6e1669fbd9e944744b7c9b2bd6c400/gdal/apps/gdalwarp_lib.cpp#L2293-L2362\n\n    Args:\n        src_dst (rasterio.io.DatasetReader or rasterio.io.DatasetWriter or rasterio.vrt.WarpedVRT): Rasterio dataset.\n        bounds (tuple): Bounding box coordinates in target crs (**dstSRS**).\n        overviews (list): List of overview decimation levels.\n        height (int): Desired output height of the array for the input bounds.\n        width (int): Desired output width of the array for the input bounds.\n        srcSRS (rasterio.crs.CRS, optional): Source Coordinate Reference System. Defaults to `epsg:4326`.\n        dstSRS (rasterio.crs.CRS, optional): Target Coordinate Reference System. Defaults to `epsg:3857`.\n\n    Returns:\n        int: Overview level.\n\n    \"\"\"\n\n    dst_transform, _, _ = calculate_default_transform(\n        srcSRS, dstSRS, dataset_shape[1], dataset_shape[0], *dataset_bounds\n    )\n    src_res = dst_transform.a\n\n    # Compute what the \"natural\" output resolution\n    # (in pixels) would be for this input dataset\n    vrt_transform = from_bounds(*target_bounds, width, height)\n    target_res = vrt_transform.a\n\n    ovr_idx = -1\n    if target_res &gt; src_res:\n        res = [src_res * decim for decim in overviews]\n\n        for idx in range(ovr_idx, len(res) - 1):\n            ovr_idx = idx\n            ovrRes = src_res if ovr_idx &lt; 0 else res[ovr_idx]\n            nextRes = res[ovr_idx + 1]\n\n            if (ovrRes &lt; target_res) and (nextRes &gt; target_res):\n                break\n\n            if abs(ovrRes - target_res) &lt; 1e-1:\n                break\n\n        else:\n            print(\"else\")\n            ovr_idx = len(res) - 1\n    return overviews[ovr_idx - 1]\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import target_extent\n\n    te = target_extent[zoom]\n\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}-overviews.zarr\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_or_create(storage=storage)\n\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Hard code some metadata that could be included in a GeoZarr spec\n    overviews = [2, 4, 8, 16, 32, 64]\n    bounds = [-179.995, -89.99499999999999, 180.005, 89.99499999999999]\n    shape = (17999, 36000)\n    # Get overview level for associated zoom level\n    level = get_overview_level(bounds, shape, te, overviews)\n    # Open overview\n    da = open_zarr(store, group=str(level), zarr_format=3, consolidated=False)[\"var\"]\n    # Set input dataset projection\n    da = da.rio.write_crs(srcSRS)\n    da = da.rio.clip_box(\n        *te,\n        crs=dstSRS,\n    )\n    # Define affine transformation from input to output dataset\n    dst_transform, w, h = calculate_default_transform(\n        srcSRS,\n        dstSRS,\n        da.rio.width,\n        da.rio.height,\n        *da.rio.bounds(),\n        dst_width=width,\n        dst_height=height,\n    )\n    # Reproject dataset\n    return da.rio.reproject(dstSRS, shape=(h, w), transform=dst_transform).load()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"mursst\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))"
  },
  {
    "objectID": "examples/resample-zarr-pyresample-zarr-icechunk.html",
    "href": "examples/resample-zarr-pyresample-zarr-icechunk.html",
    "title": "Resampling with pyresample (S3 storage, Zarr V3 store, Zarr reader with icechunk)¶",
    "section": "",
    "text": "import argparse\n\nimport xarray as xr\nfrom icechunk import IcechunkStore, StorageConfig\nfrom pyresample.area_config import create_area_def\nfrom pyresample.gradient import block_nn_interpolator, gradient_resampler_indices_block\nfrom pyresample.resampler import resample_blocks\n\n\ndef warp_resample(dataset, zoom=0):\n    from common import earthaccess_args, target_extent\n\n    te = target_extent[zoom]\n\n    # Define filepath, driver, and variable information\n    args = earthaccess_args[dataset]\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Define source and target projection\n    dstSRS = \"EPSG:3857\"\n    srcSRS = \"EPSG:4326\"\n    width = height = 256\n    # Rechunk MURSST to operate on fewer chunks\n    if dataset == \"mursst\":\n        da = da.chunk({\"lat\": 4000, \"lon\": 4000})\n    elif dataset == \"gpm_imerg\":\n        # Transpose dims to align with pyresample expectations\n        da = da.squeeze()\n    # Create area definition for the target dataset\n    target_area_def = create_area_def(\n        area_id=1,\n        projection=dstSRS,\n        shape=(height, width),\n        area_extent=te,\n    )\n    # Create area definition for the source dataset\n    source_area_def = create_area_def(\n        area_id=2,\n        projection=srcSRS,\n        shape=(da.sizes[\"lat\"], da.sizes[\"lon\"]),\n        area_extent=[-179.995, 89.995, 180.005, -89.995],\n    )\n    # Compute indices for resampling\n    indices_xy = resample_blocks(\n        gradient_resampler_indices_block,\n        source_area_def,\n        [],\n        target_area_def,\n        chunk_size=(1, height, width),\n        dtype=float,\n    )\n    # Apply resampler\n    resampled = resample_blocks(\n        block_nn_interpolator,\n        source_area_def,\n        [da.data],\n        target_area_def,\n        dst_arrays=[indices_xy],\n        chunk_size=(1, height, width),\n        dtype=da.dtype,\n    )\n    # Reproject dataset\n    return resampled.compute()\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = warp_resample(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"mursst\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = warp_resample(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Pyresample",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-zarr-sparse-zarr-icechunk.html",
    "href": "examples/resample-zarr-sparse-zarr-icechunk.html",
    "title": "Resampling with sparse (S3 storage, Zarr V3 store, Zarr reader with icechunk)¶",
    "section": "",
    "text": "import argparse\nimport warnings\n\nimport numpy as np\nimport xarray as xr\nfrom common import earthaccess_args\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef xr_regridder(\n    ds: xr.Dataset,\n    grid: xr.Dataset,\n    weights: xr.DataArray,\n) -&gt; xr.Dataset:\n    \"\"\"\n    Xarray-aware regridding function that uses weights from xESMF but performs the regridding using sparse matrix multiplication.\n\n    Parameters\n    ----------\n    ds\n    weights\n    out_grid_shape\n\n    Returns\n    -------\n    regridded_ds\n\n    Notes\n    -----\n    Modified from https://github.com/carbonplan/ndpyramid/pull/130\n    \"\"\"\n\n    latlon_dims = [\"lat\", \"lon\"]\n\n    shape_in = (ds.sizes[\"lat\"], ds.sizes[\"lon\"])\n    shape_out = (grid.sizes[\"y\"], grid.sizes[\"x\"])\n\n    regridded_ds = xr.apply_ufunc(\n        esmf_apply_weights,\n        weights,\n        ds,\n        input_core_dims=[[\"out_dim\", \"in_dim\"], latlon_dims],\n        output_core_dims=[latlon_dims],\n        exclude_dims=set(latlon_dims),\n        kwargs={\"shape_in\": shape_in, \"shape_out\": shape_out},\n        keep_attrs=True,\n    )\n\n    return regridded_ds\n\n\ndef esmf_apply_weights(weights, indata, shape_in, shape_out):\n    \"\"\"\n    Apply regridding weights to data.\n    Parameters\n    ----------\n    A : scipy sparse COO matrix\n    indata : numpy array of shape ``(..., n_lat, n_lon)`` or ``(..., n_y, n_x)``.\n        Should be C-ordered. Will be then tranposed to F-ordered.\n    shape_in, shape_out : tuple of two integers\n        Input/output data shape for unflatten operation.\n        For rectilinear grid, it is just ``(n_lat, n_lon)``.\n    Returns\n    -------\n    outdata : numpy array of shape ``(..., shape_out[0], shape_out[1])``.\n        Extra dimensions are the same as `indata`.\n        If input data is C-ordered, output will also be C-ordered.\n    Notes\n    -----\n    From https://github.com/carbonplan/ndpyramid/pull/130\n    \"\"\"\n\n    # COO matrix is fast with F-ordered array but slow with C-array, so we\n    # take in a C-ordered and then transpose)\n    # (CSR or CRS matrix is fast with C-ordered array but slow with F-array)\n    if not indata.flags[\"C_CONTIGUOUS\"]:\n        warnings.warn(\"Input array is not C_CONTIGUOUS. \" \"Will affect performance.\")\n\n    # get input shape information\n    shape_horiz = indata.shape[-2:]\n    extra_shape = indata.shape[0:-2]\n\n    assert shape_horiz == shape_in, (\n        \"The horizontal shape of input data is {}, different from that of\"\n        \"the regridder {}!\".format(shape_horiz, shape_in)\n    )\n\n    assert (\n        shape_in[0] * shape_in[1] == weights.shape[1]\n    ), \"ny_in * nx_in should equal to weights.shape[1]\"\n\n    assert (\n        shape_out[0] * shape_out[1] == weights.shape[0]\n    ), \"ny_out * nx_out should equal to weights.shape[0]\"\n\n    # use flattened array for dot operation\n    indata_flat = indata.reshape(-1, shape_in[0] * shape_in[1])\n    outdata_flat = weights.dot(indata_flat.T).T\n\n    # unflattened output array\n    outdata = outdata_flat.reshape([*extra_shape, shape_out[0], shape_out[1]])\n    return outdata\n\n\ndef regrid(dataset, zoom=0):\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-weights-{zoom}\",\n        region=\"us-west-2\",\n    )\n    target_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-target-{zoom}\",\n        region=\"us-west-2\",\n    )\n    weights_store = IcechunkStore.open_existing(storage=weights_storage, mode=\"r\")\n    target_store = IcechunkStore.open_existing(storage=target_storage, mode=\"r\")\n    weights = _reconstruct_xesmf_weights(\n        xr.open_zarr(weights_store, zarr_format=3, consolidated=False)\n    )\n    grid = xr.open_zarr(target_store, zarr_format=3, consolidated=False).load()\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]].load()\n    return xr_regridder(da, grid, weights)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "Sparse (with pre-generated weights)",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/resample-zarr-xesmfcached-zarr-icechunk.html",
    "href": "examples/resample-zarr-xesmfcached-zarr-icechunk.html",
    "title": "Resampling with XESMF (S3 storage, Zarr V3 store, Zarr reader with icechunk)¶",
    "section": "",
    "text": "import argparse\n\nimport numpy as np\nimport xarray as xr\nimport xesmf as xe\nfrom icechunk import IcechunkStore, StorageConfig\n\n\ndef _reconstruct_xesmf_weights(ds_w):\n    \"\"\"\n    Reconstruct weights into format that xESMF understands\n\n    Notes\n    -----\n    From ndpyramid - https://github.com/carbonplan/ndpyramid\n    \"\"\"\n    import sparse\n    import xarray as xr\n\n    col = ds_w[\"col\"].values - 1\n    row = ds_w[\"row\"].values - 1\n    s = ds_w[\"S\"].values\n    n_out, n_in = ds_w.attrs[\"n_out\"], ds_w.attrs[\"n_in\"]\n    crds = np.stack([row, col])\n    return xr.DataArray(\n        sparse.COO(crds, s, (n_out, n_in)), dims=(\"out_dim\", \"in_dim\"), name=\"weights\"\n    )\n\n\ndef regrid(dataset, zoom=0):\n    from common import earthaccess_args  # noqa: 402\n\n    args = earthaccess_args[dataset]\n    # Load pre-generated weights and target dataset\n    weights_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-weights-{zoom}\",\n        region=\"us-west-2\",\n    )\n    target_storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/test-weight-caching/{dataset}-target-{zoom}\",\n        region=\"us-west-2\",\n    )\n    weights_store = IcechunkStore.open_existing(storage=weights_storage, mode=\"r\")\n    target_store = IcechunkStore.open_existing(storage=target_storage, mode=\"r\")\n    weights = _reconstruct_xesmf_weights(\n        xr.open_zarr(weights_store, zarr_format=3, consolidated=False)\n    )\n    grid = xr.open_zarr(target_store, zarr_format=3, consolidated=False)\n    # Open dataset\n    storage = StorageConfig.s3_from_env(\n        bucket=\"nasa-veda-scratch\",\n        prefix=f\"resampling/icechunk/{dataset}\",\n        region=\"us-west-2\",\n    )\n    store = IcechunkStore.open_existing(storage=storage, mode=\"r\")\n    da = xr.open_zarr(store, zarr_format=3, consolidated=False)[args[\"variable\"]]\n    # Create XESMF regridder\n    regridder = xe.Regridder(\n        da,\n        grid,\n        \"nearest_s2d\",\n        periodic=True,\n        extrap_method=\"nearest_s2d\",\n        ignore_degenerate=True,\n        reuse_weights=True,\n        weights=weights,\n    )\n    # Regrid dataset\n    return regridder(da)\n\n\nif __name__ == \"__main__\":\n    if \"get_ipython\" in dir():\n        # Just call warp_resample if running as a Jupyter Notebook\n        da = regrid(\"gpm_imerg\")\n    else:\n        # Configure dataset via argpase if running via CLI\n        parser = argparse.ArgumentParser(description=\"Set environment for the script.\")\n        parser.add_argument(\n            \"--dataset\",\n            default=\"gpm_imerg\",\n            help=\"Dataset to resample.\",\n            choices=[\"gpm_imerg\", \"mursst\"],\n        )\n        parser.add_argument(\n            \"--zoom\",\n            default=0,\n            help=\"Zoom level for tile extent.\",\n        )\n        user_args = parser.parse_args()\n        da = regrid(user_args.dataset, int(user_args.zoom))",
    "crumbs": [
      "Resampling libraries",
      "XESMF (with pre-generated weights)",
      "Zarr Reader (S3 storage)"
    ]
  },
  {
    "objectID": "examples/run-pyinstrument.html",
    "href": "examples/run-pyinstrument.html",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "import subprocess\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Literal\n\nimport fsspec\nfrom utils import sync_notebook\n\n\nfs = fsspec.filesystem(\"file\")\ncurrent_date = datetime.today().strftime(\"%Y-%m-%d\")\noutput_folder = f\"results/{current_date}/\"\nfs.mkdirs(output_folder, exist_ok=True)\n\n\ndataset = \"gpm_imerg\"\n\n\ndef run_pyinstrument(file: str, zoom: str, output_format: Literal[\"json\", \"html\"]):\n    output_file = f\"{output_folder}pyinstrument-{dataset}-{Path(file).stem}-{zoom}.{output_format}\"\n    command = [\n        \"pyinstrument\",\n        \"--outfile\",\n        output_file,\n        \"-r\",\n        output_format,\n        file,\n        \"--dataset\",\n        dataset,\n        \"--zoom\",\n        zoom,\n    ]\n    subprocess.run(command)\n\n\ndef sync_and_run(zoom, input_methods):\n    notebooks = []\n    for fp in input_methods:\n        notebooks.extend(fs.glob(f\"resample-*-{fp}*.ipynb\"))\n    for file in notebooks:\n        sync_notebook(file)\n    modules = []\n    for fp in input_methods:\n        modules.extend(fs.glob(f\"resample-*-{fp}*.py\"))\n    for file in modules:\n        # Skip kerchunk since it requires a different image build due to incompatibility with Zarr V3\n        if \"kerchunk\" not in file:\n            run_pyinstrument(file, zoom, \"json\")\n\n\nif dataset == \"gpm_imerg\":\n    input_methods = [\"rioxarray\", \"odc\", \"pyresample\", \"rioxarray\", \"xesmf\", \"sparse\"]\n    zoom_levels = [\"0\", \"1\", \"2\"]\nelif dataset == \"mursst\":\n    input_methods = [\"odc\", \"pyresample\", \"rioxarray\", \"rasterio\"]\n    zoom_levels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n\n\nfor zoom in zoom_levels:\n    sync_and_run(zoom, input_methods)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "This excalidraw diagram shouws the components involved in a resampling workflow, along with the available Python implementations in 2024.\n\n\n\n\n\nValcke S, Piacentini A, Jonville G. Benchmarking Regridding Libraries Used in Earth System Modelling. Mathematical and Computational Applications. 2022; 27(2):31. https://doi.org/10.3390/mca27020031\n\n\n\n\n\nECMFF Interpolation Design document\nNCAR’s climate data guide\nGeocomputation with Python\n\n\n\n\n\nWhat’s Next - Software - Regridding\nLazy regridding discussion\nInterpolation and regridding\nConservative Region Aggregation with Xarray, Geopandas, and Sparse\nCan a reprojection/change of CRS opearation be done lazily using rioxarray?]\n\n\n\n\n\nXIOS\nSCRIP\nYAC\nATLAS\nclimate4R\ntempestremap",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#resources",
    "href": "resources.html#resources",
    "title": "Geospatial reprojection in Python (2024)",
    "section": "",
    "text": "This excalidraw diagram shouws the components involved in a resampling workflow, along with the available Python implementations in 2024.\n\n\n\n\n\nValcke S, Piacentini A, Jonville G. Benchmarking Regridding Libraries Used in Earth System Modelling. Mathematical and Computational Applications. 2022; 27(2):31. https://doi.org/10.3390/mca27020031\n\n\n\n\n\nECMFF Interpolation Design document\nNCAR’s climate data guide\nGeocomputation with Python\n\n\n\n\n\nWhat’s Next - Software - Regridding\nLazy regridding discussion\nInterpolation and regridding\nConservative Region Aggregation with Xarray, Geopandas, and Sparse\nCan a reprojection/change of CRS opearation be done lazily using rioxarray?]\n\n\n\n\n\nXIOS\nSCRIP\nYAC\nATLAS\nclimate4R\ntempestremap",
    "crumbs": [
      "Resources"
    ]
  }
]